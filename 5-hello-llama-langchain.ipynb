{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7ec54e-589f-4f5f-880d-5f4b18d0bceb",
   "metadata": {},
   "source": [
    "# 5 Hello, Llama: langchain  \n",
    "\n",
    "_LangChain is a framework for developing applications powered by large language models (LLMs)._ [https://github.com/langchain-ai/langchain]  \n",
    "It helps developers build complex, multi-step workflows by connecting LLMs with external data sources, such as databases or APIs, and organizing those interactions into modular components.  \n",
    "LangChain enables tasks like prompt generation, data retrieval, and decision-making through the use of chains (sequences of actions) and agents (autonomous decision-makers), making it a versatile tool for applications such as chatbots, question-answering systems, and other AI-driven workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbf50e-0974-4d82-bf6c-5fab22e54d53",
   "metadata": {},
   "source": [
    "#### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371f9314-05fc-48ed-be73-7619909845fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install required libraries\n",
    "\n",
    "# !pip install langchain_huggingface==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e1ed0-a7ed-4a4f-adef-bcff17ca8db8",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a6f18d-bce3-40d4-8def-3200352fa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec8fce-1fac-4da3-8c42-4dc1ee43b573",
   "metadata": {},
   "source": [
    "#### Choose the model you want to use\n",
    "\n",
    "The model could be downloaded from HuggingFace for example here --> https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct. You can clone the repo locally after creating an account on Huggingface and accepting the meta policies.  \n",
    "\n",
    "_Note: you can configure transformer library to download it without cloning repo manually._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e62531d-21e2-4d59-bba4-3b337c77b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following folder to point the path where you have stored the model you want to use\n",
    "base_folder = \"FILL_WITH_BASE_FOLDER\" # Example: \"C:/Users/username/Documents/HuggingFace\"\n",
    "\n",
    "model_name = \"Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# set the model id\n",
    "model_id = os.path.join(base_folder, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146dc7b-467c-4c6b-988a-ba5c7f3c54e4",
   "metadata": {},
   "source": [
    "#### Build HuggingFace pipeline to be used in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d73018-f0e0-41f6-9bc4-a8d03b9301dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb163108dfd241cbb2ea1eb33e52e9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=128, \n",
    "    top_k=50, \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(\n",
    "    pipeline=pipe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b70b0-37ed-45d3-aa44-5adac3c440f6",
   "metadata": {},
   "source": [
    "#### Define the prompt template\n",
    "\n",
    "In LangChain, a prompt template is a predefined structure used to format input prompts for large language models (LLMs).   \n",
    "It combines static text and dynamic variables, allowing users to customize prompts by filling in specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d60d219-7f8e-4911-acc2-3617c843aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "System: You are an expert on world capitals. Respond only to the question. And respond only if the question is related to countries capitals; otherwise, respond with \"I don't know the answer.\"\n",
    "\n",
    "Query: {query}\n",
    "Response:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fdd1f-d527-4c73-b3ba-3f75e326f057",
   "metadata": {},
   "source": [
    "#### Create chain using python pipe operator\n",
    "\n",
    "In Python, the pipe operator (|) is primarily used for two main purposes:\n",
    "- __Bitwise OR Operator__: When applied between two integers, the pipe operator performs a bitwise OR operation.\n",
    "- __Chaining Operator__ : In contexts such as the Langchain library, the pipe operator allows for chaining operations in a functional style, enhancing readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82cf0e3-1f92-4608-a7c9-10be492c788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaining prompt_template and pipeline\n",
    "chain = prompt_template | hf_pipeline.bind(skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd94ab3-eb3c-42c6-b20b-1f39696a2be9",
   "metadata": {},
   "source": [
    "#### Define input dict and execute chain steps\n",
    "\n",
    "Define the input_dict to fill the prompt_template variable defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20cd867-922a-4830-86ee-19583761409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "input_dict = {\"query\": \"What is the capital of France?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485771d1-c458-45fb-9a78-0a5f9a2528cc",
   "metadata": {},
   "source": [
    "### Check if the llm and prompt works as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb64fd-a384-4fa8-b39b-7bf6c51073f0",
   "metadata": {},
   "source": [
    "##### Bad Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc39aa2e-76cf-4778-a41e-064e7679896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "input_dict = {\"query\": \"I know Ottawa is a capital city. But what is birmingham?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ea645a-761c-4c49-a7e5-f7c33003aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "input_dict = {\"query\": \"What is an intel core i5?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfaacdd-cd6d-452e-81e3-7391091a1ff7",
   "metadata": {},
   "source": [
    "##### Different wording of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a04280c-b67d-4236-a624-52f72725dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Ottawa is the capital city of Canada. It has been the country's capital since 1857.\n"
     ]
    }
   ],
   "source": [
    "input_dict = {\"query\": \"Is Ottawa a capital city?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc6e51a-6bc2-44c2-8e9c-eaca47163bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome is the capital of Italy.\n"
     ]
    }
   ],
   "source": [
    "input_dict = {\"query\": \"If Ottawa is the capital of Canada what is Rome?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0624ef-8590-447e-8138-14ff28d2b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ottawa is the capital of Canada.\n"
     ]
    }
   ],
   "source": [
    "# bad question but the model could correct the speaker\n",
    "input_dict = {\"query\": \"What is the capital of Ottawa?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f493f-61b7-46fe-963c-3197e3aebcea",
   "metadata": {},
   "source": [
    "## Not known answer (RAG key concept)\n",
    "\n",
    "Large Language Models (LLMs) are trained on vast amounts of data, but their knowledge is limited to the information available up to their training cutoff. So, what happens if a new country is created after this period?\n",
    "\n",
    "In such cases, the model may not know about new developments. To address this, we can apply the key concept of **Retrieval-Augmented Generation (RAG)**. RAG involves providing additional, relevant information (known as \"context\") to help the model generate a more accurate response.\n",
    "\n",
    "For example, if a new state is created and the model has no prior knowledge of it, we can supply context like:\n",
    "**\"Edoras is the capital of Rohan.\"**\n",
    "\n",
    "By incorporating this context, the model can answer questions about concepts it would not otherwise know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae45df76-44f3-4dbf-a9e6-9305769b431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "# example of unkown capital\n",
    "input_dict = {\"query\": \"What is the capital of Rohan?\"}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0fb89-b88b-4d76-9bef-cfa10d68fce4",
   "metadata": {},
   "source": [
    "#### Changing the prompt to support a context and updating chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b10897e9-d22e-4534-8704-e47db2e6d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "System: You are an expert on world capitals. Respond only to the question. And respond only if the question is related to countries capitals; otherwise, respond with \"I don't know the answer.\". In addition to your knowledge consider the \"Extra information\" below. \n",
    "\n",
    "Extra information: {context}\\n\\n\n",
    "\n",
    "Query: {query}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# chaining prompt_template and pipeline\n",
    "chain = prompt_template | hf_pipeline.bind(skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576a8a5-4d23-4dd1-bd60-4f6c8884eb79",
   "metadata": {},
   "source": [
    "#### Execute the same query that fails before adding context \n",
    "\n",
    "Context that will be addes: The capital of Rohan is the fortified town of Edoras, on a hill in a valley of the White Mountains. \"Edoras\" is Old English for \"enclosures\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d38127-9190-4f11-a5c4-284f4b5baa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {\"context\": \"The capital of Rohan is the fortified town of Edoras, on a hill in a valley of the White Mountains. \\\"Edoras\\\" is Old English for \\\"enclosures\\\".\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9d65f05-b42a-49de-a670-37b7f9100e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edoras\n"
     ]
    }
   ],
   "source": [
    "# example of unkown capital\n",
    "input_dict = {\"query\": \"What is the capital of Rohan?\", \"context\": context}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6112c99c-b299-4e2c-8958-52378ca5d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edoras is the capital of Rohan.\n"
     ]
    }
   ],
   "source": [
    "# example of unkown capital\n",
    "input_dict = {\"query\": \"What is Edoras?\", \"context\": context}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40cdfb-bcb4-4dbe-9c59-170367dab524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e39db52-bea5-4826-a92f-865a90880b9b",
   "metadata": {},
   "source": [
    "### Final cosiderations\n",
    "\n",
    "This is the basic idea behind building a Retrieval-Augmented Generation (RAG) system. However, the key challenge lies in how to efficiently find and retrieve the right context to include in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d00f6-1a2e-4b8b-8604-fe2dd4f0a5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
