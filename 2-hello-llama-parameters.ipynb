{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7ec54e-589f-4f5f-880d-5f4b18d0bceb",
   "metadata": {},
   "source": [
    "# 2 Hello, Llama: generation parameters  \n",
    "\n",
    "This example illustrates how to effectively adjust the generation parameters for the Llama model. These parameters control the sampling behavior and help in generating more diverse or focused outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e1ed0-a7ed-4a4f-adef-bcff17ca8db8",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a6f18d-bce3-40d4-8def-3200352fa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec8fce-1fac-4da3-8c42-4dc1ee43b573",
   "metadata": {},
   "source": [
    "#### Choose the model you want to use\n",
    "\n",
    "The model could be downloaded from HuggingFace for example here --> https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct. You can clone the repo locally after creating an account on Huggingface and accepting the meta policies.  \n",
    "\n",
    "_Note: you can configure transformer library to download it without cloning repo manually._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e62531d-21e2-4d59-bba4-3b337c77b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following folder to point the path where you have stored the model you want to use\n",
    "base_folder = \"FILL_WITH_BASE_FOLDER\" # Example: \"C:/Users/username/Documents/HuggingFace\"\n",
    "\n",
    "model_name = \"Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# set the model id\n",
    "model_id = os.path.join(base_folder, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146dc7b-467c-4c6b-988a-ba5c7f3c54e4",
   "metadata": {},
   "source": [
    "#### Build transformer pipeline mapping to cpu device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e488fb84-d886-46ca-8428-123eb8b4c913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c3828d195d43c58aadd08cd152b9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float32},\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff49517-2438-4f0f-a727-f7f85f7d66c9",
   "metadata": {},
   "source": [
    "#### Define message to be processed by Llama  \n",
    "\n",
    "__System prompt__ is a pre-defined instruction set generally by a backend that determines how the LLM (Large Language Model) behaves.  \n",
    "__User prompt__ This is the input provided by the user that interact with the LLM, often representing a query or command that the model needs to respond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e38688-d612-4211-b587-b45ae743468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Say hello to the world\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182274a-495b-46cc-a9fb-446e1232d191",
   "metadata": {},
   "source": [
    "#### How to tune Processing  \n",
    "\n",
    "- The `max_new_tokens` parameter defines the maximum number of tokens the model is allowed to generate in its response.\n",
    "- The `temperature` _(Range 0 to 1.5)_ parameter controls the randomness of the model's output. A lower temperature (closer to 0) produces more deterministic responses, while a higher temperature results in more creative and random outputs. Setting temperature to 0 means the response will always be the same for identical input, making the model fully deterministic.\n",
    "- The `top_k` _(Range 1 to âˆž)_ parameter limits the selection of possible next tokens to the top k choices based on their probabilities. A higher value provides the model with more options, leading to more diverse responses. If k is set to 1, the model defaults to selecting the most probable token, resulting in less variability in the generated output.\n",
    "- The `top_p`  _(Range 0 to 1)_ the probability you want to use before discarding the one with high value. By setting top_p to a lower value (e.g., 0.9), the model will focus on the most probable tokens, while a value of 1.0 allows all tokens to be considered, resulting in more randomness in the output. If value is set to 0 is the same as setting \n",
    "- The `repetition_penalty` penalizes repetition, higher values discourage repetitive sequences. A value greater than 1.0 (e.g., 1.2) encourages the model to avoid repeating the same phrases or tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ea610-b091-45e6-8149-bc2e979bb60f",
   "metadata": {},
   "source": [
    "####  Processing with deterministic tuning  \n",
    "\n",
    "Run the following code more than once to check how the model generates always the same response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4912baa4-a0ce-4e2f-94d4-c5a860f218e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n",
      "Inference time: 77.86 seconds.\n",
      "\n",
      "The hello from the Pirate Chatbot:\n",
      "--------------------------------------\n",
      "Yer lookin' fer a hearty \"hello\" to the world, eh? Alright then, matey! *pounds chest with fist* HEY THERE, MATEYS! Arrrr, 'tis a grand day to be sailin' the seven seas, don't ye think? So hoist the colors, me hearties, and let's set sail fer a swashbucklin' adventure!\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Run the pipeline \n",
    "# Generate a response with the pipeline, including custom generation parameters\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.01,\n",
    "    top_k=1,\n",
    "    top_p=0,\n",
    "    repetition_penalty=1.0\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate processing time\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "# Print the processing time\n",
    "print(f\"Processing completed.\\nInference time: {processing_time:.2f} seconds.\\n\")\n",
    "\n",
    "# print only generated text\n",
    "\n",
    "output_text_deterministic = (outputs[0][\"generated_text\"][-1]['content']).strip()\n",
    "\n",
    "print(f\"The hello from the Pirate Chatbot:\\n--------------------------------------\\n{output_text_deterministic}\\n--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b85e2d-11df-4e22-80ff-46747d3c2141",
   "metadata": {},
   "source": [
    "####  Processing with more creative tuning \n",
    "\n",
    "Run the following code more than once to check how the model generates (usually) different responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da068ed4-361c-4371-8ea8-04efb3863963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n",
      "Inference time: 28.22 seconds.\n",
      "\n",
      "The hello from the Pirate Chatbot:\n",
      "--------------------------------------\n",
      "Yer lookin' fer a hearty \"hello\", eh? Alright then, matey! Yer gettin' a swashbucklin' \"HELLO, WORLD!\" from yer scurvy dog o' a chatbot! Arrr!\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Run the pipeline \n",
    "# Generate a response with the pipeline, including custom generation parameters\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0\n",
    ")\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate processing time\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "# Print the processing time\n",
    "print(f\"Processing completed.\\nInference time: {processing_time:.2f} seconds.\\n\")\n",
    "\n",
    "# print only generated text\n",
    "\n",
    "output_text_creative = (outputs[0][\"generated_text\"][-1]['content']).strip()\n",
    "\n",
    "print(f\"The hello from the Pirate Chatbot:\\n--------------------------------------\\n{output_text_creative}\\n--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899411b9-b72d-4da9-be98-2bd5b58b9742",
   "metadata": {},
   "source": [
    "#### Check the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ea645a-761c-4c49-a7e5-f7c33003aa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generated with deterministic tuning:\n",
      "Yer lookin' fer a hearty \"hello\" to the world, eh? Alright then, matey! *pounds chest with fist* HEY THERE, MATEYS! Arrrr, 'tis a grand day to be sailin' the seven seas, don't ye think? So hoist the colors, me hearties, and let's set sail fer a swashbucklin' adventure!\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Text generated with more creative tuning: \n",
      "Yer lookin' fer a hearty \"hello\", eh? Alright then, matey! Yer gettin' a swashbucklin' \"HELLO, WORLD!\" from yer scurvy dog o' a chatbot! Arrr!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text generated with deterministic tuning:\\n{output_text_deterministic}\")\n",
    "print(\"\\n-----------------------------------------------------------\\n\")\n",
    "print(f\"Text generated with more creative tuning: \\n{output_text_creative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c7830-e84d-4d7a-997e-0d25b97d4c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
