{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TuAgGNKt5HO"
      },
      "source": [
        "# Tutorial: Exporting StableHLO from JAX\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)][jax-tutorial-colab]\n",
        "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)][jax-tutorial-kaggle]\n",
        "\n",
        "JAX is a Python library for high-performance numerical computing. This tutorial shows how to export JAX and Flax (JAX-powered neural network library) models to StableHLO, and directly to TensorFlow SavedModel.\n",
        "\n",
        "## Tutorial Setup\n",
        "\n",
        "### Install required dependencies\n",
        "\n",
        "We use `jax` and `jaxlib` (JAX's support library with compiled binaries), along with `flax` and `transformers` for some models to export.\n",
        "We also need to install `tensorflow` to work with SavedModel, and recommend using `tensorflow-cpu` or `tf-nightly` for this tutorial.\n",
        "\n",
        "[jax-tutorial-colab]: https://colab.research.google.com/github/openxla/stablehlo/blob/main/docs/tutorials/jax-export.ipynb\n",
        "[jax-tutorial-kaggle]: https://kaggle.com/kernels/welcome?src=https://github.com/openxla/stablehlo/blob/main/docs/tutorials/jax-export.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cV4sn1bmeTgq",
        "outputId": "52a43004-e2fc-4ba6-fe67-fc00eae6704b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ENUcO6aML-Nq",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1d8cc0b0-c7ac-49b6-eca6-a33a270ad0c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jax\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Collecting flax\n",
            "  Downloading flax-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting tensorflow-cpu\n",
            "  Downloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.3)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.67)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (1.67.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow-cpu)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow-cpu)\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (3.12.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.44.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow-cpu) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.1.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.10.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-cpu) (3.0.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.20.2)\n",
            "Downloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flax-0.10.1-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.3/419.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, jaxlib, keras, jax, tensorflow-cpu, flax\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.0 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flax-0.10.1 jax-0.4.35 jaxlib-0.4.35 keras-3.6.0 tensorboard-2.18.0 tensorflow-cpu-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U jax jaxlib flax transformers tensorflow-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "HqjeC_QSugYj"
      },
      "outputs": [],
      "source": [
        "#@title Define `get_stablehlo_asm` to help with MLIR printing\n",
        "from jax._src.interpreters import mlir as jax_mlir\n",
        "from jax._src.lib.mlir import ir\n",
        "\n",
        "# Returns prettyprint of StableHLO module without large constants\n",
        "def get_stablehlo_asm(module_str):\n",
        "  with jax_mlir.make_ir_context():\n",
        "    stablehlo_module = ir.Module.parse(module_str, context=jax_mlir.make_ir_context())\n",
        "    return stablehlo_module.operation.get_asm(large_elements_limit=20)\n",
        "\n",
        "# Disable logging for better tutorial rendering\n",
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNlFj80UwX0D"
      },
      "source": [
        "_Note: This helper uses a JAX internal API that may break at any time, but it serves no functional purpose in the tutorial aside from readability._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEfsW69IBp_V"
      },
      "source": [
        "## Export JAX model to StableHLO using `jax.export`\n",
        "\n",
        "In this section we'll export a basic JAX function and a Flax model to StableHLO.\n",
        "\n",
        "The preferred API for export is [`jax.export`](https://jax.readthedocs.io/en/latest/jax.export.html#module-jax.export). The function to export must be JIT transformed, specifically a result of `jax.jit`, to be exported to StableHLO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MzMjjf2iIk2"
      },
      "source": [
        "### Export basic JAX model to StableHLO\n",
        "\n",
        "Let's start by exporting a basic `plus` function to StableHLO, using `np.int32` argument types to trace the function.\n",
        "\n",
        "Export requires specifying shapes using `jax.ShapeDtypeStruct`, which can be constructed from NumPy values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-GN3vPbvFoa",
        "outputId": "5c751b6c-2661-4ac4-9d50-1e45672dbd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_plus attributes {jax.uses_shape_polymorphism = false, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32> {jax.result_info = \"\"}) {\n",
            "    %0 = stablehlo.add %arg0, %arg1 : tensor<i32>\n",
            "    return %0 : tensor<i32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "from jax import export\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "# Create a JIT-transformed function\n",
        "@jax.jit\n",
        "def plus(x,y):\n",
        "  return jnp.add(x,y)\n",
        "\n",
        "# Create abstract input shapes\n",
        "inputs = (np.int32(1), np.int32(1),)\n",
        "input_shapes = [jax.ShapeDtypeStruct(input.shape, input.dtype) for input in inputs]\n",
        "\n",
        "# Export the function to StableHLO\n",
        "stablehlo_add = export.export(plus)(*input_shapes).mlir_module()\n",
        "print(get_stablehlo_asm(stablehlo_add))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZWVAHQzBsEM"
      },
      "source": [
        "### Export Hugging Face FlaxResNet18 to StableHLO\n",
        "\n",
        "Now let's look at a simple model that appears in the wild, `resnet18`.\n",
        "\n",
        "We'll export a `flax` model from the Hugging Face `transformers` ResNet page, [FlaxResNetModel](https://huggingface.co/docs/transformers/en/model_doc/resnet#transformers.FlaxResNetModel). This steps setup was copied from the Hugging Face documentation.\n",
        "\n",
        "The documentation also states: _\"Finally, this model supports inherent JAX features such as: **Just-In-Time (JIT) compilation** ...\"_ which means it is perfect for export.\n",
        "\n",
        "Similar to our very basic example, our steps for export are:\n",
        "\n",
        "1. Instantiate a callable (model/function)\n",
        "2. JIT-transform it with `jax.jit`\n",
        "3. Specify shapes for export using `jax.ShapeDtypeStruct` on NumPy values\n",
        "4. Use the JAX `export` API to get a StableHLO module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "380c81f382154e6ba3efe095c67b144a",
            "2b1f6ab4339e47bc9e38fb5761f18979",
            "050c8a5cde1242caabb38df30b17c1c1",
            "d5e540c6357f48a7ab3b2b94d83cd134",
            "cf356b245daf4e76ad1ce004a6484dd5",
            "87df9af2b5444bdcab73d33264963cec",
            "7b674244d0a74b9b9e6816e501b16c3e",
            "ba2bbca105b84e5fb49a47aa26cf1d48",
            "2e6483653aa74c0bb40f2fbafa1c9f11",
            "8c2352d91e674be897d4a43624ba7d63",
            "50da9b3e450d435a9df7a79dd110c2de",
            "013de714e91b4aba9ac0a16d83758ca6",
            "3921d6b491154a52aa7aaef66b4bfc7e",
            "8446d52500da43a7986ab2b7f8d03020",
            "7a06f00059f943598b78808a61892d04",
            "ea38f6b6aca94f298899c2c27d675a02",
            "0a615e90cf5d434882a55731eb7ff029",
            "d1e21d9b12d145d5ad87319a1691e1d6",
            "098f52f4561b46a89cd0d48bb9889379",
            "ff0d371c27334310b0dcf1d6496c24a3",
            "595dd5d2b8774798b4b96fcf64ae8478",
            "c9883e3cceab4370a88a1989cc17c8a2"
          ]
        },
        "id": "53T7jO-v_6PC",
        "outputId": "601b48c9-6b61-42d6-9094-335598576283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/69.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "380c81f382154e6ba3efe095c67b144a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "013de714e91b4aba9ac0a16d83758ca6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = false, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<1x3x224x224xf32>) -> (tensor<1x512x7x7xf32> {jax.result_info = \"[0]\"}, tensor<1x512x1x1xf32> {jax.result_info = \"[1]\"}) {\n",
            "    %c = stablehlo.constant dense<49> : tensor<i32>\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>\n",
            "    %cst_1 = stablehlo.constant dense<9.99999974E-6> : tensor<f32>\n",
            "    %cst_2 = stablehlo.constant dense_reso \n",
            "...\n",
            " func.func private @relu_3(%arg0: tensor<1x7x7x512xf32>) -> tensor<1x7x7x512xf32> {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1x7x7x512xf32>\n",
            "    %1 = stablehlo.maximum %arg0, %0 : tensor<1x7x7x512xf32>\n",
            "    return %1 : tensor<1x7x7x512xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoImageProcessor, FlaxResNetModel\n",
        "import jax\n",
        "import numpy as np\n",
        "\n",
        "# Construct jit-transformed flax model with sample inputs\n",
        "resnet18 = FlaxResNetModel.from_pretrained(\"microsoft/resnet-18\", return_dict=False)\n",
        "resnet18_jit = jax.jit(resnet18)\n",
        "sample_input = np.random.randn(1, 3, 224, 224)\n",
        "input_shape = jax.ShapeDtypeStruct(sample_input.shape, sample_input.dtype)\n",
        "\n",
        "# Export to StableHLO\n",
        "stablehlo_resnet18_export = export.export(resnet18_jit)(input_shape)\n",
        "resnet18_stablehlo = get_stablehlo_asm(stablehlo_resnet18_export.mlir_module())\n",
        "print(resnet18_stablehlo[:600], \"\\n...\\n\", resnet18_stablehlo[-345:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2MC9F7Zlx6E"
      },
      "source": [
        "### Export with dynamic batch size\n",
        "\n",
        "Now let's export that same model with a dynamic batch size!\n",
        "\n",
        "In the first example, we used an input shape of `tensor<1x3x224x224xf32>`, specifying strict constraints on the input shape. If we want to defer the concrete shapes used in compilation until a later point, we can specify a `symbolic_shape`. In this example, we'll export using `tensor<?x3x224x224xf32>`.\n",
        "\n",
        "Symbolic shapes are specified using `export.symbolic_shape`, with letters representing symint dimensions. For example, a valid 2-d matrix multiplication could use symbolic constraints of: `2,a * a,5` to ensure the refined program will have valid shapes. Symbolic integer names are kept track of by an `export.SymbolicScope` to avoid unintentional name clashes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIkbtViEMJ3T",
        "outputId": "165019c7-e771-43d6-c324-6bb4222798ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32>) -> (tensor<?x512x7x7xf32> {jax.result_info = \"[0]\"}, tensor<?x512x1x1xf32> {jax.result_info = \"[1]\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32>\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32>\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> ()\n",
            "    %2:2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> (tensor<?x512x7x7xf32>, tensor<?x512x1x1xf32>)\n",
            "    return %2#0, %2#1 : tensor<?x512x7x7xf32>, tensor<?x512x1x1xf32>\n",
            "  }\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"}, %arg1: tensor<?x3x224x224xf32>) -> (tensor<?x512x7x7xf32> {jax.result_info = \"[0]\"}, tensor<?x512x1x1xf32> {jax.result_info = \"[1]\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<1xi32>\n",
            "    %c_0 = stablehlo.constant dense<49> : tensor<i32>\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %c_1 = stablehlo.constant dense<512> : tensor<1xi32>\n",
            "    %c_2 = stablehlo.constant dense<7> : tensor<1xi32>\n",
            "    %c_3 = stablehlo.constant dense<256> : tensor<1x \n",
            "...\n",
            " , tensor<1xi32>) -> tensor<4xi32>\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x14x14x256xf32>\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x14x14x256xf32>\n",
            "    return %3 : tensor<?x14x14x256xf32>\n",
            "  }\n",
            "  func.func private @relu_3(%arg0: tensor<i32> {jax.global_constant = \"a\"}, %arg1: tensor<?x7x7x512xf32>) -> tensor<?x7x7x512xf32> {\n",
            "    %c = stablehlo.constant dense<512> : tensor<1xi32>\n",
            "    %c_0 = stablehlo.constant dense<7> : tensor<1xi32>\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32>\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c_0, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x7x7x512xf32>\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x7x7x512xf32>\n",
            "    return %3 : tensor<?x7x7x512xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Construct dynamic sample inputs\n",
        "dyn_scope = export.SymbolicScope()\n",
        "dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
        "\n",
        "# Export to StableHLO\n",
        "dyn_resnet18_export = export.export(resnet18_jit)(dyn_input_shape)\n",
        "dyn_resnet18_stablehlo = get_stablehlo_asm(dyn_resnet18_export.mlir_module())\n",
        "print(dyn_resnet18_stablehlo[:1900], \"\\n...\\n\", dyn_resnet18_stablehlo[-1000:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import export\n",
        "import numpy as np\n",
        "from jax.nn import relu, softmax\n",
        "\n",
        "# Define a simple CNN class\n",
        "class SimpleCNN:\n",
        "    def __init__(self):\n",
        "        # Initialize weights for convolution and dense layers\n",
        "        self.conv1_weight = jnp.ones((32, 3, 3, 3))  # 32 filters, 3x3 kernel, 3 input channels\n",
        "        self.conv2_weight = jnp.ones((64, 32, 3, 3))  # 64 filters, 3x3 kernel, 32 input channels\n",
        "\n",
        "        # Define the output size after convolutions with input (batch, 3, 224, 224)\n",
        "        # Adjusting the fully connected layer shape accordingly\n",
        "        self.fc_weight = jnp.ones((64 * 224 * 224, 10))  # Flattened feature size to 10 classes\n",
        "\n",
        "    # Define forward pass\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, self.conv1_weight, (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jax.lax.conv(x, self.conv2_weight, (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "\n",
        "        # Flatten for fully connected layer\n",
        "        x = x.reshape((x.shape[0], -1))  # Ensure flattened size matches fc_weight\n",
        "        x = jnp.dot(x, self.fc_weight)\n",
        "        return softmax(x)\n",
        "\n",
        "# Instantiate the model and apply JIT\n",
        "cnn_model = SimpleCNN()\n",
        "cnn_jit = jax.jit(cnn_model)\n",
        "\n",
        "# Define a valid dynamic input shape for Google Colab\n",
        "try:\n",
        "    # Set a dynamic input shape; 'a' is a batch size symbol, 3 is the color channel, and 224x224 is the image resolution\n",
        "    dyn_scope = export.SymbolicScope()\n",
        "    dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
        "\n",
        "    # Export the CNN model to StableHLO\n",
        "    dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
        "    dyn_cnn_stablehlo = dyn_cnn_export.mlir_module()\n",
        "\n",
        "    # Directly print the MLIR module content\n",
        "    print(\"MLIR Representation:\\n\", dyn_cnn_stablehlo)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error in generating StableHLO MLIR module:\", e)\n"
      ],
      "metadata": {
        "id": "osvOAOIQfaOv",
        "outputId": "aab9702f-92b2-4c68-fdc7-3ffc9a8a0675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLIR Representation:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-8-b72e4635e514>\":41:0)\n",
            "#loc2 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-8-b72e4635e514>\":21:0)\n",
            "#loc15 = loc(\"<ipython-input-8-b72e4635e514>\":23:0)\n",
            "#loc19 = loc(\"<cell line: 35>\"(#loc1))\n",
            "#loc20 = loc(\"run_code\"(#loc2))\n",
            "#loc21 = loc(\"run_ast_nodes\"(#loc3))\n",
            "#loc22 = loc(\"run_cell_async\"(#loc4))\n",
            "#loc23 = loc(\"_pseudo_sync_runner\"(#loc5))\n",
            "#loc24 = loc(\"_run_cell\"(#loc6))\n",
            "#loc25 = loc(\"run_cell\"(#loc7))\n",
            "#loc26 = loc(\"run_cell\"(#loc8))\n",
            "#loc27 = loc(\"do_execute\"(#loc9))\n",
            "#loc30 = loc(\"__call__\"(#loc13))\n",
            "#loc32 = loc(\"__call__\"(#loc15))\n",
            "#loc37 = loc(callsite(#loc26 at #loc27))\n",
            "#loc39 = loc(callsite(#loc25 at #loc37))\n",
            "#loc41 = loc(callsite(#loc24 at #loc39))\n",
            "#loc43 = loc(callsite(#loc23 at #loc41))\n",
            "#loc45 = loc(callsite(#loc22 at #loc43))\n",
            "#loc47 = loc(callsite(#loc21 at #loc45))\n",
            "#loc49 = loc(callsite(#loc20 at #loc47))\n",
            "#loc51 = loc(callsite(#loc19 at #loc49))\n",
            "#loc54 = loc(callsite(#loc30 at #loc51))\n",
            "#loc56 = loc(callsite(#loc32 at #loc51))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc56))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc60)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc61)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc62)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x10xf32> loc(#loc)\n",
            "    return %2 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<10> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_2 = stablehlo.constant dense<3211264> : tensor<1xi32> loc(#loc)\n",
            "    %cst_3 = stablehlo.constant dense<1.000000e+00> : tensor<32x3x3x3xf32> loc(#loc)\n",
            "    %cst_4 = stablehlo.constant dense<1.000000e+00> : tensor<64x32x3x3xf32> loc(#loc)\n",
            "    %cst_5 = stablehlo.constant dense<1.000000e+00> : tensor<3211264x10xf32> loc(#loc)\n",
            "    %0 = stablehlo.convolution(%arg1, %cst_3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<32x3x3x3xf32>) -> tensor<?x32x224x224xf32> loc(#loc63)\n",
            "    %1 = call @relu(%arg0, %0) : (tensor<i32>, tensor<?x32x224x224xf32>) -> tensor<?x32x224x224xf32> loc(#loc64)\n",
            "    %2 = stablehlo.convolution(%1, %cst_4) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x32x224x224xf32>, tensor<64x32x3x3xf32>) -> tensor<?x64x224x224xf32> loc(#loc65)\n",
            "    %3 = call @relu_0(%arg0, %2) : (tensor<i32>, tensor<?x64x224x224xf32>) -> tensor<?x64x224x224xf32> loc(#loc66)\n",
            "    %4 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc67)\n",
            "    %5 = stablehlo.concatenate %4, %c_2, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc67)\n",
            "    %6 = stablehlo.dynamic_reshape %3, %5 : (tensor<?x64x224x224xf32>, tensor<2xi32>) -> tensor<?x3211264xf32> loc(#loc67)\n",
            "    %7 = stablehlo.dot_general %6, %cst_5, contracting_dims = [1] x [0] : (tensor<?x3211264xf32>, tensor<3211264x10xf32>) -> tensor<?x10xf32> loc(#loc68)\n",
            "    %8 = stablehlo.reduce(%7 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc69)\n",
            "    %9 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %10 = stablehlo.dynamic_broadcast_in_dim %cst_1, %9, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc70)\n",
            "    %11 = stablehlo.maximum %10, %8 : tensor<?xf32> loc(#loc70)\n",
            "    %12 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc71)\n",
            "    %13 = stablehlo.concatenate %12, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc71)\n",
            "    %14 = stablehlo.dynamic_broadcast_in_dim %11, %13, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc71)\n",
            "    %15 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc72)\n",
            "    %16 = stablehlo.concatenate %15, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc72)\n",
            "    %17 = stablehlo.dynamic_broadcast_in_dim %14, %16, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc72)\n",
            "    %18 = stablehlo.subtract %7, %17 : tensor<?x10xf32> loc(#loc72)\n",
            "    %19 = stablehlo.exponential %18 : tensor<?x10xf32> loc(#loc73)\n",
            "    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc74)\n",
            "    %21 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc71)\n",
            "    %22 = stablehlo.concatenate %21, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc71)\n",
            "    %23 = stablehlo.dynamic_broadcast_in_dim %20, %22, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc71)\n",
            "    %24 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc75)\n",
            "    %25 = stablehlo.concatenate %24, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc75)\n",
            "    %26 = stablehlo.dynamic_broadcast_in_dim %23, %25, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc75)\n",
            "    %27 = stablehlo.divide %19, %26 : tensor<?x10xf32> loc(#loc75)\n",
            "    return %27 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54)), %arg1: tensor<?x32x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))) -> tensor<?x32x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<32> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc64)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc76)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc76)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x32x224x224xf32> loc(#loc76)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x32x224x224xf32> loc(#loc76)\n",
            "    return %3 : tensor<?x32x224x224xf32> loc(#loc64)\n",
            "  } loc(#loc64)\n",
            "  func.func private @relu_0(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc56)), %arg1: tensor<?x64x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc56))) -> tensor<?x64x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<64> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc66)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc77)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc77)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x64x224x224xf32> loc(#loc77)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x64x224x224xf32> loc(#loc77)\n",
            "    return %3 : tensor<?x64x224x224xf32> loc(#loc66)\n",
            "  } loc(#loc66)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\":234:0)\n",
            "#loc12 = loc(\"<ipython-input-8-b72e4635e514>\":20:0)\n",
            "#loc14 = loc(\"<ipython-input-8-b72e4635e514>\":22:0)\n",
            "#loc16 = loc(\"<ipython-input-8-b72e4635e514>\":26:0)\n",
            "#loc17 = loc(\"<ipython-input-8-b72e4635e514>\":27:0)\n",
            "#loc18 = loc(\"<ipython-input-8-b72e4635e514>\":28:0)\n",
            "#loc28 = loc(\"wrapper\"(#loc10))\n",
            "#loc29 = loc(\"__call__\"(#loc12))\n",
            "#loc31 = loc(\"__call__\"(#loc14))\n",
            "#loc33 = loc(\"__call__\"(#loc16))\n",
            "#loc34 = loc(\"__call__\"(#loc17))\n",
            "#loc35 = loc(\"__call__\"(#loc18))\n",
            "#loc36 = loc(callsite(#loc27 at #loc28))\n",
            "#loc38 = loc(callsite(#loc26 at #loc36))\n",
            "#loc40 = loc(callsite(#loc25 at #loc38))\n",
            "#loc42 = loc(callsite(#loc24 at #loc40))\n",
            "#loc44 = loc(callsite(#loc23 at #loc42))\n",
            "#loc46 = loc(callsite(#loc22 at #loc44))\n",
            "#loc48 = loc(callsite(#loc21 at #loc46))\n",
            "#loc50 = loc(callsite(#loc20 at #loc48))\n",
            "#loc52 = loc(callsite(#loc19 at #loc50))\n",
            "#loc53 = loc(callsite(#loc29 at #loc51))\n",
            "#loc55 = loc(callsite(#loc31 at #loc51))\n",
            "#loc57 = loc(callsite(#loc33 at #loc51))\n",
            "#loc58 = loc(callsite(#loc34 at #loc51))\n",
            "#loc59 = loc(callsite(#loc35 at #loc51))\n",
            "#loc60 = loc(\"/dimension_size\"(#loc52))\n",
            "#loc61 = loc(\"/ge\"(#loc52))\n",
            "#loc62 = loc(\"/shape_assertion\"(#loc52))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc53))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc55))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reshape\"(#loc57))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/dot_general\"(#loc58))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc59))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc59))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc59))\n",
            "#loc72 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc59))\n",
            "#loc73 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc59))\n",
            "#loc74 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc59))\n",
            "#loc75 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc59))\n",
            "#loc76 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc54))\n",
            "#loc77 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc56))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import export\n",
        "import numpy as np\n",
        "from jax.nn import relu, softmax\n",
        "\n",
        "# Define 10 structurally distinct models\n",
        "class CNNModel1:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((16, 3, 3, 3)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jnp.max(x, axis=(1, 2))  # Global Max Pooling\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel2:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((8, 3, 5, 5)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jnp.mean(x, axis=(1, 2))  # Global Average Pooling\n",
        "        x = x.reshape((x.shape[0], -1))  # Flatten to match fully connected layer\n",
        "        x = jnp.dot(x, jnp.ones((x.shape[-1], 10)))  # Fully connected layer with output size 10\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel3:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((32, 3, 3, 3)), (2, 2), 'VALID')\n",
        "        x = relu(x)\n",
        "        x = jax.lax.conv(x, jnp.ones((64, 32, 3, 3)), (2, 2), 'VALID')\n",
        "        x = relu(x)\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel4:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((16, 3, 7, 7)), (2, 2), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jnp.mean(x, axis=(1, 2))\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel5:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((16, 3, 3, 3)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jax.lax.conv(x, jnp.ones((32, 16, 3, 3)), (2, 2), 'VALID')\n",
        "        x = relu(x)\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        return softmax(x)\n",
        "\n",
        "class LinearModel1:\n",
        "    def __call__(self, x):\n",
        "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
        "        x = jnp.dot(x, jnp.ones((x.shape[-1], 50)))\n",
        "        x = relu(x)\n",
        "        x = jnp.dot(x, jnp.ones((50, 10)))\n",
        "        return softmax(x)\n",
        "\n",
        "class LinearModel2:\n",
        "    def __call__(self, x):\n",
        "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
        "        x = jnp.dot(x, jnp.ones((x.shape[-1], 10)))  # Output size modified to 10 for compatibility\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel6:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((128, 3, 3, 3)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jax.lax.conv(x, jnp.ones((256, 128, 3, 3)), (2, 2), 'VALID')\n",
        "        x = relu(x)\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel7:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((8, 3, 1, 1)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jax.lax.conv(x, jnp.ones((16, 8, 1, 1)), (1, 1), 'SAME')\n",
        "        x = jnp.mean(x, axis=(1, 2))\n",
        "        return softmax(x)\n",
        "\n",
        "class CNNModel8:\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.conv(x, jnp.ones((32, 3, 3, 3)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jax.lax.conv(x, jnp.ones((64, 32, 3, 3)), (1, 1), 'SAME')\n",
        "        x = relu(x)\n",
        "        x = jnp.max(x, axis=(1, 2))  # Global Max Pooling\n",
        "        flattened_dim = x.shape[-1]\n",
        "        x = jnp.dot(x, jnp.ones((flattened_dim, 10)))  # Fully connected layer\n",
        "        return softmax(x)\n",
        "\n",
        "# List of models\n",
        "models = [CNNModel1(), CNNModel2(), CNNModel3(), CNNModel4(), CNNModel5(),\n",
        "          LinearModel1(), LinearModel2(), CNNModel6(), CNNModel7(), CNNModel8()]\n",
        "\n",
        "# Define a function to print MLIR from StableHLO\n",
        "def get_mlir_output(cnn_jit, model_index):\n",
        "    try:\n",
        "        # Define a dynamic input shape; 'a' is a batch size symbol, 3 is the color channel, and 224x224 is the image resolution\n",
        "        dyn_scope = export.SymbolicScope()\n",
        "        dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
        "\n",
        "        # Export the model to StableHLO\n",
        "        dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
        "        dyn_cnn_stablehlo = dyn_cnn_export.mlir_module()\n",
        "\n",
        "        # Print the MLIR for each model\n",
        "        print(f\"MLIR for Model {model_index}:\\n\", dyn_cnn_stablehlo, \"\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generating MLIR for Model {model_index}:\", e)\n",
        "\n",
        "# Iterate over each model and export MLIR\n",
        "for i, model in enumerate(models, 1):\n",
        "    # Apply JIT compilation\n",
        "    cnn_jit = jax.jit(model)\n",
        "    get_mlir_output(cnn_jit, i)\n"
      ],
      "metadata": {
        "id": "Xy4L0XPfib3S",
        "outputId": "e310b799-6819-4677-8670-e2cfd6ad4ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLIR for Model 1:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":11:0)\n",
            "#loc16 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc17 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc18 = loc(\"run_code\"(#loc3))\n",
            "#loc19 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc20 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc21 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc22 = loc(\"_run_cell\"(#loc7))\n",
            "#loc23 = loc(\"run_cell\"(#loc8))\n",
            "#loc24 = loc(\"run_cell\"(#loc9))\n",
            "#loc27 = loc(\"__call__\"(#loc13))\n",
            "#loc31 = loc(callsite(#loc23 at #loc24))\n",
            "#loc33 = loc(callsite(#loc22 at #loc31))\n",
            "#loc35 = loc(callsite(#loc21 at #loc33))\n",
            "#loc37 = loc(callsite(#loc20 at #loc35))\n",
            "#loc39 = loc(callsite(#loc19 at #loc37))\n",
            "#loc41 = loc(callsite(#loc18 at #loc39))\n",
            "#loc43 = loc(callsite(#loc17 at #loc41))\n",
            "#loc45 = loc(callsite(#loc16 at #loc43))\n",
            "#loc48 = loc(callsite(#loc27 at #loc45))\n",
            "#loc56 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc48))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x224xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc51)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc52)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc53)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x224xf32> loc(#loc)\n",
            "    return %2 : tensor<?x224xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x224xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x3x3x3xf32> loc(#loc54)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<16x3x3x3xf32>) -> tensor<?x16x224x224xf32> loc(#loc55)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x16x224x224xf32>) -> tensor<?x16x224x224xf32> loc(#loc56)\n",
            "    %3 = stablehlo.reduce(%2 init: %cst_1) applies stablehlo.maximum across dimensions = [1, 2] : (tensor<?x16x224x224xf32>, tensor<f32>) -> tensor<?x224xf32> loc(#loc57)\n",
            "    %4 = stablehlo.reduce(%3 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<?x224xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc58)\n",
            "    %5 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc59)\n",
            "    %6 = stablehlo.dynamic_broadcast_in_dim %cst_1, %5, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc59)\n",
            "    %7 = stablehlo.maximum %6, %4 : tensor<?xf32> loc(#loc59)\n",
            "    %8 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc60)\n",
            "    %9 = stablehlo.concatenate %8, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc60)\n",
            "    %10 = stablehlo.dynamic_broadcast_in_dim %7, %9, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc60)\n",
            "    %11 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc61)\n",
            "    %12 = stablehlo.concatenate %11, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc61)\n",
            "    %13 = stablehlo.dynamic_broadcast_in_dim %10, %12, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x224xf32> loc(#loc61)\n",
            "    %14 = stablehlo.subtract %3, %13 : tensor<?x224xf32> loc(#loc61)\n",
            "    %15 = stablehlo.exponential %14 : tensor<?x224xf32> loc(#loc62)\n",
            "    %16 = stablehlo.reduce(%15 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x224xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc63)\n",
            "    %17 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc60)\n",
            "    %18 = stablehlo.concatenate %17, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc60)\n",
            "    %19 = stablehlo.dynamic_broadcast_in_dim %16, %18, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc60)\n",
            "    %20 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc64)\n",
            "    %21 = stablehlo.concatenate %20, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc64)\n",
            "    %22 = stablehlo.dynamic_broadcast_in_dim %19, %21, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x224xf32> loc(#loc64)\n",
            "    %23 = stablehlo.divide %15, %22 : tensor<?x224xf32> loc(#loc64)\n",
            "    return %23 : tensor<?x224xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc48)), %arg1: tensor<?x16x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc48))) -> tensor<?x16x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<16> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc56)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc65)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc65)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x16x224x224xf32> loc(#loc65)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x16x224x224xf32> loc(#loc65)\n",
            "    return %3 : tensor<?x16x224x224xf32> loc(#loc56)\n",
            "  } loc(#loc56)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":10:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":12:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":13:0)\n",
            "#loc25 = loc(\"do_execute\"(#loc10))\n",
            "#loc26 = loc(\"__call__\"(#loc12))\n",
            "#loc28 = loc(\"__call__\"(#loc14))\n",
            "#loc29 = loc(\"__call__\"(#loc15))\n",
            "#loc30 = loc(callsite(#loc24 at #loc25))\n",
            "#loc32 = loc(callsite(#loc23 at #loc30))\n",
            "#loc34 = loc(callsite(#loc22 at #loc32))\n",
            "#loc36 = loc(callsite(#loc21 at #loc34))\n",
            "#loc38 = loc(callsite(#loc20 at #loc36))\n",
            "#loc40 = loc(callsite(#loc19 at #loc38))\n",
            "#loc42 = loc(callsite(#loc18 at #loc40))\n",
            "#loc44 = loc(callsite(#loc17 at #loc42))\n",
            "#loc46 = loc(callsite(#loc16 at #loc44))\n",
            "#loc47 = loc(callsite(#loc26 at #loc45))\n",
            "#loc49 = loc(callsite(#loc28 at #loc45))\n",
            "#loc50 = loc(callsite(#loc29 at #loc45))\n",
            "#loc51 = loc(\"/dimension_size\"(#loc46))\n",
            "#loc52 = loc(\"/ge\"(#loc46))\n",
            "#loc53 = loc(\"/shape_assertion\"(#loc46))\n",
            "#loc54 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc47))\n",
            "#loc55 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc47))\n",
            "#loc57 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc49))\n",
            "#loc58 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc50))\n",
            "#loc59 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc50))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc50))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc50))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc50))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc50))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc50))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc48))\n",
            " \n",
            "\n",
            "MLIR for Model 2:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":18:0)\n",
            "#loc17 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc18 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc19 = loc(\"run_code\"(#loc3))\n",
            "#loc20 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc21 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc22 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc23 = loc(\"_run_cell\"(#loc7))\n",
            "#loc24 = loc(\"run_cell\"(#loc8))\n",
            "#loc25 = loc(\"run_cell\"(#loc9))\n",
            "#loc28 = loc(\"__call__\"(#loc13))\n",
            "#loc33 = loc(callsite(#loc24 at #loc25))\n",
            "#loc35 = loc(callsite(#loc23 at #loc33))\n",
            "#loc37 = loc(callsite(#loc22 at #loc35))\n",
            "#loc39 = loc(callsite(#loc21 at #loc37))\n",
            "#loc41 = loc(callsite(#loc20 at #loc39))\n",
            "#loc43 = loc(callsite(#loc19 at #loc41))\n",
            "#loc45 = loc(callsite(#loc18 at #loc43))\n",
            "#loc47 = loc(callsite(#loc17 at #loc45))\n",
            "#loc50 = loc(callsite(#loc28 at #loc47))\n",
            "#loc59 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc50))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc54)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc55)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc56)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x10xf32> loc(#loc)\n",
            "    return %2 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<10> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_1 = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.792000e+03> : tensor<f32> loc(#loc)\n",
            "    %cst_3 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %cst_4 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<8x3x5x5xf32> loc(#loc57)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[2, 2], [2, 2]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<8x3x5x5xf32>) -> tensor<?x8x224x224xf32> loc(#loc58)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x8x224x224xf32>) -> tensor<?x8x224x224xf32> loc(#loc59)\n",
            "    %3 = stablehlo.reduce(%2 init: %cst_3) applies stablehlo.add across dimensions = [1, 2] : (tensor<?x8x224x224xf32>, tensor<f32>) -> tensor<?x224xf32> loc(#loc60)\n",
            "    %4 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc61)\n",
            "    %5 = stablehlo.concatenate %4, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc61)\n",
            "    %6 = stablehlo.dynamic_broadcast_in_dim %cst_2, %5, dims = [] : (tensor<f32>, tensor<2xi32>) -> tensor<?x224xf32> loc(#loc61)\n",
            "    %7 = stablehlo.divide %3, %6 : tensor<?x224xf32> loc(#loc61)\n",
            "    %8 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<224x10xf32> loc(#loc62)\n",
            "    %9 = stablehlo.dot_general %7, %8, contracting_dims = [1] x [0] : (tensor<?x224xf32>, tensor<224x10xf32>) -> tensor<?x10xf32> loc(#loc63)\n",
            "    %10 = stablehlo.reduce(%9 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc64)\n",
            "    %11 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc65)\n",
            "    %12 = stablehlo.dynamic_broadcast_in_dim %cst, %11, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc65)\n",
            "    %13 = stablehlo.maximum %12, %10 : tensor<?xf32> loc(#loc65)\n",
            "    %14 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %15 = stablehlo.concatenate %14, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %16 = stablehlo.dynamic_broadcast_in_dim %13, %15, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc66)\n",
            "    %17 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc67)\n",
            "    %18 = stablehlo.concatenate %17, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc67)\n",
            "    %19 = stablehlo.dynamic_broadcast_in_dim %16, %18, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc67)\n",
            "    %20 = stablehlo.subtract %9, %19 : tensor<?x10xf32> loc(#loc67)\n",
            "    %21 = stablehlo.exponential %20 : tensor<?x10xf32> loc(#loc68)\n",
            "    %22 = stablehlo.reduce(%21 init: %cst_3) applies stablehlo.add across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc69)\n",
            "    %23 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %24 = stablehlo.concatenate %23, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %25 = stablehlo.dynamic_broadcast_in_dim %22, %24, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc66)\n",
            "    %26 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %27 = stablehlo.concatenate %26, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc70)\n",
            "    %28 = stablehlo.dynamic_broadcast_in_dim %25, %27, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc70)\n",
            "    %29 = stablehlo.divide %21, %28 : tensor<?x10xf32> loc(#loc70)\n",
            "    return %29 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc50)), %arg1: tensor<?x8x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc50))) -> tensor<?x8x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<8> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc59)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc71)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc71)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x8x224x224xf32> loc(#loc71)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x8x224x224xf32> loc(#loc71)\n",
            "    return %3 : tensor<?x8x224x224xf32> loc(#loc59)\n",
            "  } loc(#loc59)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":17:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":19:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":21:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":22:0)\n",
            "#loc26 = loc(\"do_execute\"(#loc10))\n",
            "#loc27 = loc(\"__call__\"(#loc12))\n",
            "#loc29 = loc(\"__call__\"(#loc14))\n",
            "#loc30 = loc(\"__call__\"(#loc15))\n",
            "#loc31 = loc(\"__call__\"(#loc16))\n",
            "#loc32 = loc(callsite(#loc25 at #loc26))\n",
            "#loc34 = loc(callsite(#loc24 at #loc32))\n",
            "#loc36 = loc(callsite(#loc23 at #loc34))\n",
            "#loc38 = loc(callsite(#loc22 at #loc36))\n",
            "#loc40 = loc(callsite(#loc21 at #loc38))\n",
            "#loc42 = loc(callsite(#loc20 at #loc40))\n",
            "#loc44 = loc(callsite(#loc19 at #loc42))\n",
            "#loc46 = loc(callsite(#loc18 at #loc44))\n",
            "#loc48 = loc(callsite(#loc17 at #loc46))\n",
            "#loc49 = loc(callsite(#loc27 at #loc47))\n",
            "#loc51 = loc(callsite(#loc29 at #loc47))\n",
            "#loc52 = loc(callsite(#loc30 at #loc47))\n",
            "#loc53 = loc(callsite(#loc31 at #loc47))\n",
            "#loc54 = loc(\"/dimension_size\"(#loc48))\n",
            "#loc55 = loc(\"/ge\"(#loc48))\n",
            "#loc56 = loc(\"/shape_assertion\"(#loc48))\n",
            "#loc57 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc49))\n",
            "#loc58 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc49))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc51))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc51))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc52))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/dot_general\"(#loc52))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc53))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc53))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc53))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc53))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc53))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc50))\n",
            " \n",
            "\n",
            "MLIR for Model 3:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":27:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":29:0)\n",
            "#loc18 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc19 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc20 = loc(\"run_code\"(#loc3))\n",
            "#loc21 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc22 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc23 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc24 = loc(\"_run_cell\"(#loc7))\n",
            "#loc25 = loc(\"run_cell\"(#loc8))\n",
            "#loc26 = loc(\"run_cell\"(#loc9))\n",
            "#loc29 = loc(\"__call__\"(#loc13))\n",
            "#loc31 = loc(\"__call__\"(#loc15))\n",
            "#loc35 = loc(callsite(#loc25 at #loc26))\n",
            "#loc37 = loc(callsite(#loc24 at #loc35))\n",
            "#loc39 = loc(callsite(#loc23 at #loc37))\n",
            "#loc41 = loc(callsite(#loc22 at #loc39))\n",
            "#loc43 = loc(callsite(#loc21 at #loc41))\n",
            "#loc45 = loc(callsite(#loc20 at #loc43))\n",
            "#loc47 = loc(callsite(#loc19 at #loc45))\n",
            "#loc49 = loc(callsite(#loc18 at #loc47))\n",
            "#loc52 = loc(callsite(#loc29 at #loc49))\n",
            "#loc54 = loc(callsite(#loc31 at #loc49))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x193600xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc57)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc58)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc59)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x193600xf32> loc(#loc)\n",
            "    return %2 : tensor<?x193600xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x193600xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_1 = stablehlo.constant dense<193600> : tensor<1xi32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<32x3x3x3xf32> loc(#loc60)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<32x3x3x3xf32>) -> tensor<?x32x111x111xf32> loc(#loc61)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x32x111x111xf32>) -> tensor<?x32x111x111xf32> loc(#loc62)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<64x32x3x3xf32> loc(#loc63)\n",
            "    %4 = stablehlo.convolution(%2, %3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x32x111x111xf32>, tensor<64x32x3x3xf32>) -> tensor<?x64x55x55xf32> loc(#loc64)\n",
            "    %5 = call @relu_0(%arg0, %4) : (tensor<i32>, tensor<?x64x55x55xf32>) -> tensor<?x64x55x55xf32> loc(#loc65)\n",
            "    %6 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %7 = stablehlo.concatenate %6, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %8 = stablehlo.dynamic_reshape %5, %7 : (tensor<?x64x55x55xf32>, tensor<2xi32>) -> tensor<?x193600xf32> loc(#loc66)\n",
            "    %9 = stablehlo.reduce(%8 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<?x193600xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc67)\n",
            "    %10 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc68)\n",
            "    %11 = stablehlo.dynamic_broadcast_in_dim %cst_0, %10, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc68)\n",
            "    %12 = stablehlo.maximum %11, %9 : tensor<?xf32> loc(#loc68)\n",
            "    %13 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %14 = stablehlo.concatenate %13, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %15 = stablehlo.dynamic_broadcast_in_dim %12, %14, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc69)\n",
            "    %16 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %17 = stablehlo.concatenate %16, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc70)\n",
            "    %18 = stablehlo.dynamic_broadcast_in_dim %15, %17, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x193600xf32> loc(#loc70)\n",
            "    %19 = stablehlo.subtract %8, %18 : tensor<?x193600xf32> loc(#loc70)\n",
            "    %20 = stablehlo.exponential %19 : tensor<?x193600xf32> loc(#loc71)\n",
            "    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x193600xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc72)\n",
            "    %22 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %23 = stablehlo.concatenate %22, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %24 = stablehlo.dynamic_broadcast_in_dim %21, %23, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc69)\n",
            "    %25 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc73)\n",
            "    %26 = stablehlo.concatenate %25, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc73)\n",
            "    %27 = stablehlo.dynamic_broadcast_in_dim %24, %26, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x193600xf32> loc(#loc73)\n",
            "    %28 = stablehlo.divide %20, %27 : tensor<?x193600xf32> loc(#loc73)\n",
            "    return %28 : tensor<?x193600xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52)), %arg1: tensor<?x32x111x111xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52))) -> tensor<?x32x111x111xf32> {\n",
            "    %c = stablehlo.constant dense<111> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<32> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc62)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc74)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc74)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x32x111x111xf32> loc(#loc74)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x32x111x111xf32> loc(#loc74)\n",
            "    return %3 : tensor<?x32x111x111xf32> loc(#loc62)\n",
            "  } loc(#loc62)\n",
            "  func.func private @relu_0(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54)), %arg1: tensor<?x64x55x55xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))) -> tensor<?x64x55x55xf32> {\n",
            "    %c = stablehlo.constant dense<55> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<64> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc65)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc75)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc75)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x64x55x55xf32> loc(#loc75)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x64x55x55xf32> loc(#loc75)\n",
            "    return %3 : tensor<?x64x55x55xf32> loc(#loc65)\n",
            "  } loc(#loc65)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":26:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":28:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":30:0)\n",
            "#loc17 = loc(\"<ipython-input-2-2e537abe9551>\":31:0)\n",
            "#loc27 = loc(\"do_execute\"(#loc10))\n",
            "#loc28 = loc(\"__call__\"(#loc12))\n",
            "#loc30 = loc(\"__call__\"(#loc14))\n",
            "#loc32 = loc(\"__call__\"(#loc16))\n",
            "#loc33 = loc(\"__call__\"(#loc17))\n",
            "#loc34 = loc(callsite(#loc26 at #loc27))\n",
            "#loc36 = loc(callsite(#loc25 at #loc34))\n",
            "#loc38 = loc(callsite(#loc24 at #loc36))\n",
            "#loc40 = loc(callsite(#loc23 at #loc38))\n",
            "#loc42 = loc(callsite(#loc22 at #loc40))\n",
            "#loc44 = loc(callsite(#loc21 at #loc42))\n",
            "#loc46 = loc(callsite(#loc20 at #loc44))\n",
            "#loc48 = loc(callsite(#loc19 at #loc46))\n",
            "#loc50 = loc(callsite(#loc18 at #loc48))\n",
            "#loc51 = loc(callsite(#loc28 at #loc49))\n",
            "#loc53 = loc(callsite(#loc30 at #loc49))\n",
            "#loc55 = loc(callsite(#loc32 at #loc49))\n",
            "#loc56 = loc(callsite(#loc33 at #loc49))\n",
            "#loc57 = loc(\"/dimension_size\"(#loc50))\n",
            "#loc58 = loc(\"/ge\"(#loc50))\n",
            "#loc59 = loc(\"/shape_assertion\"(#loc50))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc51))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc51))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reshape\"(#loc55))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc56))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc56))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc56))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc56))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc56))\n",
            "#loc72 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc56))\n",
            "#loc73 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc56))\n",
            "#loc74 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc52))\n",
            "#loc75 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc54))\n",
            " \n",
            "\n",
            "MLIR for Model 4:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":36:0)\n",
            "#loc16 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc17 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc18 = loc(\"run_code\"(#loc3))\n",
            "#loc19 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc20 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc21 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc22 = loc(\"_run_cell\"(#loc7))\n",
            "#loc23 = loc(\"run_cell\"(#loc8))\n",
            "#loc24 = loc(\"run_cell\"(#loc9))\n",
            "#loc27 = loc(\"__call__\"(#loc13))\n",
            "#loc31 = loc(callsite(#loc23 at #loc24))\n",
            "#loc33 = loc(callsite(#loc22 at #loc31))\n",
            "#loc35 = loc(callsite(#loc21 at #loc33))\n",
            "#loc37 = loc(callsite(#loc20 at #loc35))\n",
            "#loc39 = loc(callsite(#loc19 at #loc37))\n",
            "#loc41 = loc(callsite(#loc18 at #loc39))\n",
            "#loc43 = loc(callsite(#loc17 at #loc41))\n",
            "#loc45 = loc(callsite(#loc16 at #loc43))\n",
            "#loc48 = loc(callsite(#loc27 at #loc45))\n",
            "#loc56 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc48))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x112xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc51)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc52)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc53)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x112xf32> loc(#loc)\n",
            "    return %2 : tensor<?x112xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x112xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<112> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<1.792000e+03> : tensor<f32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %cst_3 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x3x7x7xf32> loc(#loc54)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2], pad = [[2, 3], [2, 3]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<16x3x7x7xf32>) -> tensor<?x16x112x112xf32> loc(#loc55)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x16x112x112xf32>) -> tensor<?x16x112x112xf32> loc(#loc56)\n",
            "    %3 = stablehlo.reduce(%2 init: %cst_2) applies stablehlo.add across dimensions = [1, 2] : (tensor<?x16x112x112xf32>, tensor<f32>) -> tensor<?x112xf32> loc(#loc57)\n",
            "    %4 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc58)\n",
            "    %5 = stablehlo.concatenate %4, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc58)\n",
            "    %6 = stablehlo.dynamic_broadcast_in_dim %cst_1, %5, dims = [] : (tensor<f32>, tensor<2xi32>) -> tensor<?x112xf32> loc(#loc58)\n",
            "    %7 = stablehlo.divide %3, %6 : tensor<?x112xf32> loc(#loc58)\n",
            "    %8 = stablehlo.reduce(%7 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<?x112xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc59)\n",
            "    %9 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc60)\n",
            "    %10 = stablehlo.dynamic_broadcast_in_dim %cst, %9, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc60)\n",
            "    %11 = stablehlo.maximum %10, %8 : tensor<?xf32> loc(#loc60)\n",
            "    %12 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc61)\n",
            "    %13 = stablehlo.concatenate %12, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc61)\n",
            "    %14 = stablehlo.dynamic_broadcast_in_dim %11, %13, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc61)\n",
            "    %15 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc62)\n",
            "    %16 = stablehlo.concatenate %15, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc62)\n",
            "    %17 = stablehlo.dynamic_broadcast_in_dim %14, %16, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x112xf32> loc(#loc62)\n",
            "    %18 = stablehlo.subtract %7, %17 : tensor<?x112xf32> loc(#loc62)\n",
            "    %19 = stablehlo.exponential %18 : tensor<?x112xf32> loc(#loc63)\n",
            "    %20 = stablehlo.reduce(%19 init: %cst_2) applies stablehlo.add across dimensions = [1] : (tensor<?x112xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc64)\n",
            "    %21 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc61)\n",
            "    %22 = stablehlo.concatenate %21, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc61)\n",
            "    %23 = stablehlo.dynamic_broadcast_in_dim %20, %22, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc61)\n",
            "    %24 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc65)\n",
            "    %25 = stablehlo.concatenate %24, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc65)\n",
            "    %26 = stablehlo.dynamic_broadcast_in_dim %23, %25, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x112xf32> loc(#loc65)\n",
            "    %27 = stablehlo.divide %19, %26 : tensor<?x112xf32> loc(#loc65)\n",
            "    return %27 : tensor<?x112xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc48)), %arg1: tensor<?x16x112x112xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc48))) -> tensor<?x16x112x112xf32> {\n",
            "    %c = stablehlo.constant dense<112> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<16> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc56)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc66)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x16x112x112xf32> loc(#loc66)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x16x112x112xf32> loc(#loc66)\n",
            "    return %3 : tensor<?x16x112x112xf32> loc(#loc56)\n",
            "  } loc(#loc56)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":35:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":37:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":38:0)\n",
            "#loc25 = loc(\"do_execute\"(#loc10))\n",
            "#loc26 = loc(\"__call__\"(#loc12))\n",
            "#loc28 = loc(\"__call__\"(#loc14))\n",
            "#loc29 = loc(\"__call__\"(#loc15))\n",
            "#loc30 = loc(callsite(#loc24 at #loc25))\n",
            "#loc32 = loc(callsite(#loc23 at #loc30))\n",
            "#loc34 = loc(callsite(#loc22 at #loc32))\n",
            "#loc36 = loc(callsite(#loc21 at #loc34))\n",
            "#loc38 = loc(callsite(#loc20 at #loc36))\n",
            "#loc40 = loc(callsite(#loc19 at #loc38))\n",
            "#loc42 = loc(callsite(#loc18 at #loc40))\n",
            "#loc44 = loc(callsite(#loc17 at #loc42))\n",
            "#loc46 = loc(callsite(#loc16 at #loc44))\n",
            "#loc47 = loc(callsite(#loc26 at #loc45))\n",
            "#loc49 = loc(callsite(#loc28 at #loc45))\n",
            "#loc50 = loc(callsite(#loc29 at #loc45))\n",
            "#loc51 = loc(\"/dimension_size\"(#loc46))\n",
            "#loc52 = loc(\"/ge\"(#loc46))\n",
            "#loc53 = loc(\"/shape_assertion\"(#loc46))\n",
            "#loc54 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc47))\n",
            "#loc55 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc47))\n",
            "#loc57 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc49))\n",
            "#loc58 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc49))\n",
            "#loc59 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc50))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc50))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc50))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc50))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc50))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc50))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc50))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc48))\n",
            " \n",
            "\n",
            "MLIR for Model 5:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":43:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":45:0)\n",
            "#loc18 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc19 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc20 = loc(\"run_code\"(#loc3))\n",
            "#loc21 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc22 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc23 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc24 = loc(\"_run_cell\"(#loc7))\n",
            "#loc25 = loc(\"run_cell\"(#loc8))\n",
            "#loc26 = loc(\"run_cell\"(#loc9))\n",
            "#loc29 = loc(\"__call__\"(#loc13))\n",
            "#loc31 = loc(\"__call__\"(#loc15))\n",
            "#loc35 = loc(callsite(#loc25 at #loc26))\n",
            "#loc37 = loc(callsite(#loc24 at #loc35))\n",
            "#loc39 = loc(callsite(#loc23 at #loc37))\n",
            "#loc41 = loc(callsite(#loc22 at #loc39))\n",
            "#loc43 = loc(callsite(#loc21 at #loc41))\n",
            "#loc45 = loc(callsite(#loc20 at #loc43))\n",
            "#loc47 = loc(callsite(#loc19 at #loc45))\n",
            "#loc49 = loc(callsite(#loc18 at #loc47))\n",
            "#loc52 = loc(callsite(#loc29 at #loc49))\n",
            "#loc54 = loc(callsite(#loc31 at #loc49))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x394272xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc57)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc58)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc59)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x394272xf32> loc(#loc)\n",
            "    return %2 : tensor<?x394272xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x394272xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_1 = stablehlo.constant dense<394272> : tensor<1xi32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x3x3x3xf32> loc(#loc60)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<16x3x3x3xf32>) -> tensor<?x16x224x224xf32> loc(#loc61)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x16x224x224xf32>) -> tensor<?x16x224x224xf32> loc(#loc62)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<32x16x3x3xf32> loc(#loc63)\n",
            "    %4 = stablehlo.convolution(%2, %3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x16x224x224xf32>, tensor<32x16x3x3xf32>) -> tensor<?x32x111x111xf32> loc(#loc64)\n",
            "    %5 = call @relu_0(%arg0, %4) : (tensor<i32>, tensor<?x32x111x111xf32>) -> tensor<?x32x111x111xf32> loc(#loc65)\n",
            "    %6 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %7 = stablehlo.concatenate %6, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %8 = stablehlo.dynamic_reshape %5, %7 : (tensor<?x32x111x111xf32>, tensor<2xi32>) -> tensor<?x394272xf32> loc(#loc66)\n",
            "    %9 = stablehlo.reduce(%8 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<?x394272xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc67)\n",
            "    %10 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc68)\n",
            "    %11 = stablehlo.dynamic_broadcast_in_dim %cst_0, %10, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc68)\n",
            "    %12 = stablehlo.maximum %11, %9 : tensor<?xf32> loc(#loc68)\n",
            "    %13 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %14 = stablehlo.concatenate %13, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %15 = stablehlo.dynamic_broadcast_in_dim %12, %14, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc69)\n",
            "    %16 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %17 = stablehlo.concatenate %16, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc70)\n",
            "    %18 = stablehlo.dynamic_broadcast_in_dim %15, %17, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x394272xf32> loc(#loc70)\n",
            "    %19 = stablehlo.subtract %8, %18 : tensor<?x394272xf32> loc(#loc70)\n",
            "    %20 = stablehlo.exponential %19 : tensor<?x394272xf32> loc(#loc71)\n",
            "    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x394272xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc72)\n",
            "    %22 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %23 = stablehlo.concatenate %22, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %24 = stablehlo.dynamic_broadcast_in_dim %21, %23, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc69)\n",
            "    %25 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc73)\n",
            "    %26 = stablehlo.concatenate %25, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc73)\n",
            "    %27 = stablehlo.dynamic_broadcast_in_dim %24, %26, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x394272xf32> loc(#loc73)\n",
            "    %28 = stablehlo.divide %20, %27 : tensor<?x394272xf32> loc(#loc73)\n",
            "    return %28 : tensor<?x394272xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52)), %arg1: tensor<?x16x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52))) -> tensor<?x16x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<16> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc62)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc74)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc74)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x16x224x224xf32> loc(#loc74)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x16x224x224xf32> loc(#loc74)\n",
            "    return %3 : tensor<?x16x224x224xf32> loc(#loc62)\n",
            "  } loc(#loc62)\n",
            "  func.func private @relu_0(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54)), %arg1: tensor<?x32x111x111xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))) -> tensor<?x32x111x111xf32> {\n",
            "    %c = stablehlo.constant dense<111> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<32> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc65)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc75)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc75)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x32x111x111xf32> loc(#loc75)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x32x111x111xf32> loc(#loc75)\n",
            "    return %3 : tensor<?x32x111x111xf32> loc(#loc65)\n",
            "  } loc(#loc65)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":42:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":44:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":46:0)\n",
            "#loc17 = loc(\"<ipython-input-2-2e537abe9551>\":47:0)\n",
            "#loc27 = loc(\"do_execute\"(#loc10))\n",
            "#loc28 = loc(\"__call__\"(#loc12))\n",
            "#loc30 = loc(\"__call__\"(#loc14))\n",
            "#loc32 = loc(\"__call__\"(#loc16))\n",
            "#loc33 = loc(\"__call__\"(#loc17))\n",
            "#loc34 = loc(callsite(#loc26 at #loc27))\n",
            "#loc36 = loc(callsite(#loc25 at #loc34))\n",
            "#loc38 = loc(callsite(#loc24 at #loc36))\n",
            "#loc40 = loc(callsite(#loc23 at #loc38))\n",
            "#loc42 = loc(callsite(#loc22 at #loc40))\n",
            "#loc44 = loc(callsite(#loc21 at #loc42))\n",
            "#loc46 = loc(callsite(#loc20 at #loc44))\n",
            "#loc48 = loc(callsite(#loc19 at #loc46))\n",
            "#loc50 = loc(callsite(#loc18 at #loc48))\n",
            "#loc51 = loc(callsite(#loc28 at #loc49))\n",
            "#loc53 = loc(callsite(#loc30 at #loc49))\n",
            "#loc55 = loc(callsite(#loc32 at #loc49))\n",
            "#loc56 = loc(callsite(#loc33 at #loc49))\n",
            "#loc57 = loc(\"/dimension_size\"(#loc50))\n",
            "#loc58 = loc(\"/ge\"(#loc50))\n",
            "#loc59 = loc(\"/shape_assertion\"(#loc50))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc51))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc51))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reshape\"(#loc55))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc56))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc56))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc56))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc56))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc56))\n",
            "#loc72 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc56))\n",
            "#loc73 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc56))\n",
            "#loc74 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc52))\n",
            "#loc75 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc54))\n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n",
            "<ipython-input-2-2e537abe9551>:99: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_scope = export.SymbolicScope()\n",
            "<ipython-input-2-2e537abe9551>:100: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_input_shape = jax.ShapeDtypeStruct(export.symbolic_shape(\"a,3,224,224\", scope=dyn_scope), np.float32)\n",
            "<ipython-input-2-2e537abe9551>:103: DeprecationWarning: The jax.experimental.export module is deprecated. Use jax.export instead. See the migration guide at https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export.\n",
            "  dyn_cnn_export = export.export(cnn_jit)(dyn_input_shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLIR for Model 6:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":53:0)\n",
            "#loc17 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc18 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc19 = loc(\"run_code\"(#loc3))\n",
            "#loc20 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc21 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc22 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc23 = loc(\"_run_cell\"(#loc7))\n",
            "#loc24 = loc(\"run_cell\"(#loc8))\n",
            "#loc25 = loc(\"run_cell\"(#loc9))\n",
            "#loc29 = loc(\"__call__\"(#loc14))\n",
            "#loc33 = loc(callsite(#loc24 at #loc25))\n",
            "#loc35 = loc(callsite(#loc23 at #loc33))\n",
            "#loc37 = loc(callsite(#loc22 at #loc35))\n",
            "#loc39 = loc(callsite(#loc21 at #loc37))\n",
            "#loc41 = loc(callsite(#loc20 at #loc39))\n",
            "#loc43 = loc(callsite(#loc19 at #loc41))\n",
            "#loc45 = loc(callsite(#loc18 at #loc43))\n",
            "#loc47 = loc(callsite(#loc17 at #loc45))\n",
            "#loc51 = loc(callsite(#loc29 at #loc47))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc51))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc54)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc55)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc56)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x10xf32> loc(#loc)\n",
            "    return %2 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<10> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c_3 = stablehlo.constant dense<150528> : tensor<1xi32> loc(#loc)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc57)\n",
            "    %1 = stablehlo.concatenate %0, %c_3, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc57)\n",
            "    %2 = stablehlo.dynamic_reshape %arg1, %1 : (tensor<?x3x224x224xf32>, tensor<2xi32>) -> tensor<?x150528xf32> loc(#loc57)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<150528x50xf32> loc(#loc58)\n",
            "    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<?x150528xf32>, tensor<150528x50xf32>) -> tensor<?x50xf32> loc(#loc59)\n",
            "    %5 = call @relu(%arg0, %4) : (tensor<i32>, tensor<?x50xf32>) -> tensor<?x50xf32> loc(#loc60)\n",
            "    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<50x10xf32> loc(#loc61)\n",
            "    %7 = stablehlo.dot_general %5, %6, contracting_dims = [1] x [0] : (tensor<?x50xf32>, tensor<50x10xf32>) -> tensor<?x10xf32> loc(#loc62)\n",
            "    %8 = stablehlo.reduce(%7 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc63)\n",
            "    %9 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc64)\n",
            "    %10 = stablehlo.dynamic_broadcast_in_dim %cst_1, %9, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc64)\n",
            "    %11 = stablehlo.maximum %10, %8 : tensor<?xf32> loc(#loc64)\n",
            "    %12 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc65)\n",
            "    %13 = stablehlo.concatenate %12, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc65)\n",
            "    %14 = stablehlo.dynamic_broadcast_in_dim %11, %13, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc65)\n",
            "    %15 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %16 = stablehlo.concatenate %15, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %17 = stablehlo.dynamic_broadcast_in_dim %14, %16, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc66)\n",
            "    %18 = stablehlo.subtract %7, %17 : tensor<?x10xf32> loc(#loc66)\n",
            "    %19 = stablehlo.exponential %18 : tensor<?x10xf32> loc(#loc67)\n",
            "    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc68)\n",
            "    %21 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc65)\n",
            "    %22 = stablehlo.concatenate %21, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc65)\n",
            "    %23 = stablehlo.dynamic_broadcast_in_dim %20, %22, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc65)\n",
            "    %24 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %25 = stablehlo.concatenate %24, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %26 = stablehlo.dynamic_broadcast_in_dim %23, %25, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc69)\n",
            "    %27 = stablehlo.divide %19, %26 : tensor<?x10xf32> loc(#loc69)\n",
            "    return %27 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc51)), %arg1: tensor<?x50xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc51))) -> tensor<?x50xf32> {\n",
            "    %c = stablehlo.constant dense<50> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc60)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %1 = stablehlo.concatenate %0, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc70)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<2xi32>) -> tensor<?x50xf32> loc(#loc70)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x50xf32> loc(#loc70)\n",
            "    return %3 : tensor<?x50xf32> loc(#loc60)\n",
            "  } loc(#loc60)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":51:0)\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":52:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":54:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":55:0)\n",
            "#loc26 = loc(\"do_execute\"(#loc10))\n",
            "#loc27 = loc(\"__call__\"(#loc12))\n",
            "#loc28 = loc(\"__call__\"(#loc13))\n",
            "#loc30 = loc(\"__call__\"(#loc15))\n",
            "#loc31 = loc(\"__call__\"(#loc16))\n",
            "#loc32 = loc(callsite(#loc25 at #loc26))\n",
            "#loc34 = loc(callsite(#loc24 at #loc32))\n",
            "#loc36 = loc(callsite(#loc23 at #loc34))\n",
            "#loc38 = loc(callsite(#loc22 at #loc36))\n",
            "#loc40 = loc(callsite(#loc21 at #loc38))\n",
            "#loc42 = loc(callsite(#loc20 at #loc40))\n",
            "#loc44 = loc(callsite(#loc19 at #loc42))\n",
            "#loc46 = loc(callsite(#loc18 at #loc44))\n",
            "#loc48 = loc(callsite(#loc17 at #loc46))\n",
            "#loc49 = loc(callsite(#loc27 at #loc47))\n",
            "#loc50 = loc(callsite(#loc28 at #loc47))\n",
            "#loc52 = loc(callsite(#loc30 at #loc47))\n",
            "#loc53 = loc(callsite(#loc31 at #loc47))\n",
            "#loc54 = loc(\"/dimension_size\"(#loc48))\n",
            "#loc55 = loc(\"/ge\"(#loc48))\n",
            "#loc56 = loc(\"/shape_assertion\"(#loc48))\n",
            "#loc57 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reshape\"(#loc49))\n",
            "#loc58 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc50))\n",
            "#loc59 = loc(\"jit(<unnamed wrapped function>)/jit(main)/dot_general\"(#loc50))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc52))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/dot_general\"(#loc52))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc53))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc53))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc53))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc53))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc53))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc53))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc51))\n",
            " \n",
            "\n",
            "MLIR for Model 7:\n",
            " #loc = loc(unknown)\n",
            "#loc11 = loc(\"x\")\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc48)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc49)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc50)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x10xf32> loc(#loc)\n",
            "    return %2 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<10> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c_3 = stablehlo.constant dense<150528> : tensor<1xi32> loc(#loc)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc51)\n",
            "    %1 = stablehlo.concatenate %0, %c_3, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc51)\n",
            "    %2 = stablehlo.dynamic_reshape %arg1, %1 : (tensor<?x3x224x224xf32>, tensor<2xi32>) -> tensor<?x150528xf32> loc(#loc51)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<150528x10xf32> loc(#loc52)\n",
            "    %4 = stablehlo.dot_general %2, %3, contracting_dims = [1] x [0] : (tensor<?x150528xf32>, tensor<150528x10xf32>) -> tensor<?x10xf32> loc(#loc53)\n",
            "    %5 = stablehlo.reduce(%4 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc54)\n",
            "    %6 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc55)\n",
            "    %7 = stablehlo.dynamic_broadcast_in_dim %cst_1, %6, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc55)\n",
            "    %8 = stablehlo.maximum %7, %5 : tensor<?xf32> loc(#loc55)\n",
            "    %9 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc56)\n",
            "    %10 = stablehlo.concatenate %9, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc56)\n",
            "    %11 = stablehlo.dynamic_broadcast_in_dim %8, %10, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc56)\n",
            "    %12 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc57)\n",
            "    %13 = stablehlo.concatenate %12, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc57)\n",
            "    %14 = stablehlo.dynamic_broadcast_in_dim %11, %13, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc57)\n",
            "    %15 = stablehlo.subtract %4, %14 : tensor<?x10xf32> loc(#loc57)\n",
            "    %16 = stablehlo.exponential %15 : tensor<?x10xf32> loc(#loc58)\n",
            "    %17 = stablehlo.reduce(%16 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc59)\n",
            "    %18 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc56)\n",
            "    %19 = stablehlo.concatenate %18, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc56)\n",
            "    %20 = stablehlo.dynamic_broadcast_in_dim %17, %19, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc56)\n",
            "    %21 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc60)\n",
            "    %22 = stablehlo.concatenate %21, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc60)\n",
            "    %23 = stablehlo.dynamic_broadcast_in_dim %20, %22, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc60)\n",
            "    %24 = stablehlo.divide %16, %23 : tensor<?x10xf32> loc(#loc60)\n",
            "    return %24 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "} loc(#loc)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":59:0)\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":60:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":61:0)\n",
            "#loc15 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc16 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc17 = loc(\"run_code\"(#loc3))\n",
            "#loc18 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc19 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc20 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc21 = loc(\"_run_cell\"(#loc7))\n",
            "#loc22 = loc(\"run_cell\"(#loc8))\n",
            "#loc23 = loc(\"run_cell\"(#loc9))\n",
            "#loc24 = loc(\"do_execute\"(#loc10))\n",
            "#loc25 = loc(\"__call__\"(#loc12))\n",
            "#loc26 = loc(\"__call__\"(#loc13))\n",
            "#loc27 = loc(\"__call__\"(#loc14))\n",
            "#loc28 = loc(callsite(#loc23 at #loc24))\n",
            "#loc29 = loc(callsite(#loc22 at #loc23))\n",
            "#loc30 = loc(callsite(#loc22 at #loc28))\n",
            "#loc31 = loc(callsite(#loc21 at #loc29))\n",
            "#loc32 = loc(callsite(#loc21 at #loc30))\n",
            "#loc33 = loc(callsite(#loc20 at #loc31))\n",
            "#loc34 = loc(callsite(#loc20 at #loc32))\n",
            "#loc35 = loc(callsite(#loc19 at #loc33))\n",
            "#loc36 = loc(callsite(#loc19 at #loc34))\n",
            "#loc37 = loc(callsite(#loc18 at #loc35))\n",
            "#loc38 = loc(callsite(#loc18 at #loc36))\n",
            "#loc39 = loc(callsite(#loc17 at #loc37))\n",
            "#loc40 = loc(callsite(#loc17 at #loc38))\n",
            "#loc41 = loc(callsite(#loc16 at #loc39))\n",
            "#loc42 = loc(callsite(#loc16 at #loc40))\n",
            "#loc43 = loc(callsite(#loc15 at #loc41))\n",
            "#loc44 = loc(callsite(#loc15 at #loc42))\n",
            "#loc45 = loc(callsite(#loc25 at #loc43))\n",
            "#loc46 = loc(callsite(#loc26 at #loc43))\n",
            "#loc47 = loc(callsite(#loc27 at #loc43))\n",
            "#loc48 = loc(\"/dimension_size\"(#loc44))\n",
            "#loc49 = loc(\"/ge\"(#loc44))\n",
            "#loc50 = loc(\"/shape_assertion\"(#loc44))\n",
            "#loc51 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reshape\"(#loc45))\n",
            "#loc52 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc46))\n",
            "#loc53 = loc(\"jit(<unnamed wrapped function>)/jit(main)/dot_general\"(#loc46))\n",
            "#loc54 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc47))\n",
            "#loc55 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc47))\n",
            "#loc56 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc47))\n",
            "#loc57 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc47))\n",
            "#loc58 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc47))\n",
            "#loc59 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc47))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc47))\n",
            " \n",
            "\n",
            "MLIR for Model 8:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":66:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":68:0)\n",
            "#loc18 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc19 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc20 = loc(\"run_code\"(#loc3))\n",
            "#loc21 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc22 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc23 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc24 = loc(\"_run_cell\"(#loc7))\n",
            "#loc25 = loc(\"run_cell\"(#loc8))\n",
            "#loc26 = loc(\"run_cell\"(#loc9))\n",
            "#loc29 = loc(\"__call__\"(#loc13))\n",
            "#loc31 = loc(\"__call__\"(#loc15))\n",
            "#loc35 = loc(callsite(#loc25 at #loc26))\n",
            "#loc37 = loc(callsite(#loc24 at #loc35))\n",
            "#loc39 = loc(callsite(#loc23 at #loc37))\n",
            "#loc41 = loc(callsite(#loc22 at #loc39))\n",
            "#loc43 = loc(callsite(#loc21 at #loc41))\n",
            "#loc45 = loc(callsite(#loc20 at #loc43))\n",
            "#loc47 = loc(callsite(#loc19 at #loc45))\n",
            "#loc49 = loc(callsite(#loc18 at #loc47))\n",
            "#loc52 = loc(callsite(#loc29 at #loc49))\n",
            "#loc54 = loc(callsite(#loc31 at #loc49))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x3154176xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc57)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc58)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc59)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x3154176xf32> loc(#loc)\n",
            "    return %2 : tensor<?x3154176xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x3154176xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_1 = stablehlo.constant dense<3154176> : tensor<1xi32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128x3x3x3xf32> loc(#loc60)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<128x3x3x3xf32>) -> tensor<?x128x224x224xf32> loc(#loc61)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x128x224x224xf32>) -> tensor<?x128x224x224xf32> loc(#loc62)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<256x128x3x3xf32> loc(#loc63)\n",
            "    %4 = stablehlo.convolution(%2, %3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [2, 2]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x128x224x224xf32>, tensor<256x128x3x3xf32>) -> tensor<?x256x111x111xf32> loc(#loc64)\n",
            "    %5 = call @relu_0(%arg0, %4) : (tensor<i32>, tensor<?x256x111x111xf32>) -> tensor<?x256x111x111xf32> loc(#loc65)\n",
            "    %6 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %7 = stablehlo.concatenate %6, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %8 = stablehlo.dynamic_reshape %5, %7 : (tensor<?x256x111x111xf32>, tensor<2xi32>) -> tensor<?x3154176xf32> loc(#loc66)\n",
            "    %9 = stablehlo.reduce(%8 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<?x3154176xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc67)\n",
            "    %10 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc68)\n",
            "    %11 = stablehlo.dynamic_broadcast_in_dim %cst_0, %10, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc68)\n",
            "    %12 = stablehlo.maximum %11, %9 : tensor<?xf32> loc(#loc68)\n",
            "    %13 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %14 = stablehlo.concatenate %13, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %15 = stablehlo.dynamic_broadcast_in_dim %12, %14, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc69)\n",
            "    %16 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %17 = stablehlo.concatenate %16, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc70)\n",
            "    %18 = stablehlo.dynamic_broadcast_in_dim %15, %17, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x3154176xf32> loc(#loc70)\n",
            "    %19 = stablehlo.subtract %8, %18 : tensor<?x3154176xf32> loc(#loc70)\n",
            "    %20 = stablehlo.exponential %19 : tensor<?x3154176xf32> loc(#loc71)\n",
            "    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x3154176xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc72)\n",
            "    %22 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc69)\n",
            "    %23 = stablehlo.concatenate %22, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc69)\n",
            "    %24 = stablehlo.dynamic_broadcast_in_dim %21, %23, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc69)\n",
            "    %25 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc73)\n",
            "    %26 = stablehlo.concatenate %25, %c_1, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc73)\n",
            "    %27 = stablehlo.dynamic_broadcast_in_dim %24, %26, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x3154176xf32> loc(#loc73)\n",
            "    %28 = stablehlo.divide %20, %27 : tensor<?x3154176xf32> loc(#loc73)\n",
            "    return %28 : tensor<?x3154176xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52)), %arg1: tensor<?x128x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc52))) -> tensor<?x128x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<128> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc62)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc74)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc74)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x128x224x224xf32> loc(#loc74)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x128x224x224xf32> loc(#loc74)\n",
            "    return %3 : tensor<?x128x224x224xf32> loc(#loc62)\n",
            "  } loc(#loc62)\n",
            "  func.func private @relu_0(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54)), %arg1: tensor<?x256x111x111xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))) -> tensor<?x256x111x111xf32> {\n",
            "    %c = stablehlo.constant dense<111> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<256> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc65)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc75)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc75)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x256x111x111xf32> loc(#loc75)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x256x111x111xf32> loc(#loc75)\n",
            "    return %3 : tensor<?x256x111x111xf32> loc(#loc65)\n",
            "  } loc(#loc65)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":65:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":67:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":69:0)\n",
            "#loc17 = loc(\"<ipython-input-2-2e537abe9551>\":70:0)\n",
            "#loc27 = loc(\"do_execute\"(#loc10))\n",
            "#loc28 = loc(\"__call__\"(#loc12))\n",
            "#loc30 = loc(\"__call__\"(#loc14))\n",
            "#loc32 = loc(\"__call__\"(#loc16))\n",
            "#loc33 = loc(\"__call__\"(#loc17))\n",
            "#loc34 = loc(callsite(#loc26 at #loc27))\n",
            "#loc36 = loc(callsite(#loc25 at #loc34))\n",
            "#loc38 = loc(callsite(#loc24 at #loc36))\n",
            "#loc40 = loc(callsite(#loc23 at #loc38))\n",
            "#loc42 = loc(callsite(#loc22 at #loc40))\n",
            "#loc44 = loc(callsite(#loc21 at #loc42))\n",
            "#loc46 = loc(callsite(#loc20 at #loc44))\n",
            "#loc48 = loc(callsite(#loc19 at #loc46))\n",
            "#loc50 = loc(callsite(#loc18 at #loc48))\n",
            "#loc51 = loc(callsite(#loc28 at #loc49))\n",
            "#loc53 = loc(callsite(#loc30 at #loc49))\n",
            "#loc55 = loc(callsite(#loc32 at #loc49))\n",
            "#loc56 = loc(callsite(#loc33 at #loc49))\n",
            "#loc57 = loc(\"/dimension_size\"(#loc50))\n",
            "#loc58 = loc(\"/ge\"(#loc50))\n",
            "#loc59 = loc(\"/shape_assertion\"(#loc50))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc51))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc51))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reshape\"(#loc55))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc56))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc56))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc56))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc56))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc56))\n",
            "#loc72 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc56))\n",
            "#loc73 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc56))\n",
            "#loc74 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc52))\n",
            "#loc75 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc54))\n",
            " \n",
            "\n",
            "MLIR for Model 9:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":75:0)\n",
            "#loc17 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc18 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc19 = loc(\"run_code\"(#loc3))\n",
            "#loc20 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc21 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc22 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc23 = loc(\"_run_cell\"(#loc7))\n",
            "#loc24 = loc(\"run_cell\"(#loc8))\n",
            "#loc25 = loc(\"run_cell\"(#loc9))\n",
            "#loc28 = loc(\"__call__\"(#loc13))\n",
            "#loc33 = loc(callsite(#loc24 at #loc25))\n",
            "#loc35 = loc(callsite(#loc23 at #loc33))\n",
            "#loc37 = loc(callsite(#loc22 at #loc35))\n",
            "#loc39 = loc(callsite(#loc21 at #loc37))\n",
            "#loc41 = loc(callsite(#loc20 at #loc39))\n",
            "#loc43 = loc(callsite(#loc19 at #loc41))\n",
            "#loc45 = loc(callsite(#loc18 at #loc43))\n",
            "#loc47 = loc(callsite(#loc17 at #loc45))\n",
            "#loc50 = loc(callsite(#loc28 at #loc47))\n",
            "#loc59 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc50))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x224xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc54)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc55)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc56)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x224xf32> loc(#loc)\n",
            "    return %2 : tensor<?x224xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x224xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<3.584000e+03> : tensor<f32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %cst_3 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<8x3x1x1xf32> loc(#loc57)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<8x3x1x1xf32>) -> tensor<?x8x224x224xf32> loc(#loc58)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x8x224x224xf32>) -> tensor<?x8x224x224xf32> loc(#loc59)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x8x1x1xf32> loc(#loc60)\n",
            "    %4 = stablehlo.convolution(%2, %3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x8x224x224xf32>, tensor<16x8x1x1xf32>) -> tensor<?x16x224x224xf32> loc(#loc61)\n",
            "    %5 = stablehlo.reduce(%4 init: %cst_2) applies stablehlo.add across dimensions = [1, 2] : (tensor<?x16x224x224xf32>, tensor<f32>) -> tensor<?x224xf32> loc(#loc62)\n",
            "    %6 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc63)\n",
            "    %7 = stablehlo.concatenate %6, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc63)\n",
            "    %8 = stablehlo.dynamic_broadcast_in_dim %cst_1, %7, dims = [] : (tensor<f32>, tensor<2xi32>) -> tensor<?x224xf32> loc(#loc63)\n",
            "    %9 = stablehlo.divide %5, %8 : tensor<?x224xf32> loc(#loc63)\n",
            "    %10 = stablehlo.reduce(%9 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<?x224xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc64)\n",
            "    %11 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc65)\n",
            "    %12 = stablehlo.dynamic_broadcast_in_dim %cst, %11, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc65)\n",
            "    %13 = stablehlo.maximum %12, %10 : tensor<?xf32> loc(#loc65)\n",
            "    %14 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %15 = stablehlo.concatenate %14, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %16 = stablehlo.dynamic_broadcast_in_dim %13, %15, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc66)\n",
            "    %17 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc67)\n",
            "    %18 = stablehlo.concatenate %17, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc67)\n",
            "    %19 = stablehlo.dynamic_broadcast_in_dim %16, %18, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x224xf32> loc(#loc67)\n",
            "    %20 = stablehlo.subtract %9, %19 : tensor<?x224xf32> loc(#loc67)\n",
            "    %21 = stablehlo.exponential %20 : tensor<?x224xf32> loc(#loc68)\n",
            "    %22 = stablehlo.reduce(%21 init: %cst_2) applies stablehlo.add across dimensions = [1] : (tensor<?x224xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc69)\n",
            "    %23 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc66)\n",
            "    %24 = stablehlo.concatenate %23, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc66)\n",
            "    %25 = stablehlo.dynamic_broadcast_in_dim %22, %24, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc66)\n",
            "    %26 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc70)\n",
            "    %27 = stablehlo.concatenate %26, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc70)\n",
            "    %28 = stablehlo.dynamic_broadcast_in_dim %25, %27, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x224xf32> loc(#loc70)\n",
            "    %29 = stablehlo.divide %21, %28 : tensor<?x224xf32> loc(#loc70)\n",
            "    return %29 : tensor<?x224xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc50)), %arg1: tensor<?x8x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc50))) -> tensor<?x8x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<8> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc59)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc71)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc71)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x8x224x224xf32> loc(#loc71)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x8x224x224xf32> loc(#loc71)\n",
            "    return %3 : tensor<?x8x224x224xf32> loc(#loc59)\n",
            "  } loc(#loc59)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":74:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":76:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":77:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":78:0)\n",
            "#loc26 = loc(\"do_execute\"(#loc10))\n",
            "#loc27 = loc(\"__call__\"(#loc12))\n",
            "#loc29 = loc(\"__call__\"(#loc14))\n",
            "#loc30 = loc(\"__call__\"(#loc15))\n",
            "#loc31 = loc(\"__call__\"(#loc16))\n",
            "#loc32 = loc(callsite(#loc25 at #loc26))\n",
            "#loc34 = loc(callsite(#loc24 at #loc32))\n",
            "#loc36 = loc(callsite(#loc23 at #loc34))\n",
            "#loc38 = loc(callsite(#loc22 at #loc36))\n",
            "#loc40 = loc(callsite(#loc21 at #loc38))\n",
            "#loc42 = loc(callsite(#loc20 at #loc40))\n",
            "#loc44 = loc(callsite(#loc19 at #loc42))\n",
            "#loc46 = loc(callsite(#loc18 at #loc44))\n",
            "#loc48 = loc(callsite(#loc17 at #loc46))\n",
            "#loc49 = loc(callsite(#loc27 at #loc47))\n",
            "#loc51 = loc(callsite(#loc29 at #loc47))\n",
            "#loc52 = loc(callsite(#loc30 at #loc47))\n",
            "#loc53 = loc(callsite(#loc31 at #loc47))\n",
            "#loc54 = loc(\"/dimension_size\"(#loc48))\n",
            "#loc55 = loc(\"/ge\"(#loc48))\n",
            "#loc56 = loc(\"/shape_assertion\"(#loc48))\n",
            "#loc57 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc49))\n",
            "#loc58 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc49))\n",
            "#loc60 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc51))\n",
            "#loc61 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc51))\n",
            "#loc62 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc52))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc52))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc53))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc53))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc53))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc53))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc53))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc50))\n",
            " \n",
            "\n",
            "MLIR for Model 10:\n",
            " #loc = loc(unknown)\n",
            "#loc1 = loc(\"<ipython-input-2-2e537abe9551>\":103:0)\n",
            "#loc2 = loc(\"<ipython-input-2-2e537abe9551>\":115:0)\n",
            "#loc3 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3553:0)\n",
            "#loc4 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3473:0)\n",
            "#loc5 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3257:0)\n",
            "#loc6 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\":78:0)\n",
            "#loc7 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":3030:0)\n",
            "#loc8 = loc(\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\":2975:0)\n",
            "#loc9 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\":539:0)\n",
            "#loc11 = loc(\"x\")\n",
            "#loc13 = loc(\"<ipython-input-2-2e537abe9551>\":83:0)\n",
            "#loc15 = loc(\"<ipython-input-2-2e537abe9551>\":85:0)\n",
            "#loc19 = loc(\"get_mlir_output\"(#loc1))\n",
            "#loc20 = loc(\"<cell line: 112>\"(#loc2))\n",
            "#loc21 = loc(\"run_code\"(#loc3))\n",
            "#loc22 = loc(\"run_ast_nodes\"(#loc4))\n",
            "#loc23 = loc(\"run_cell_async\"(#loc5))\n",
            "#loc24 = loc(\"_pseudo_sync_runner\"(#loc6))\n",
            "#loc25 = loc(\"_run_cell\"(#loc7))\n",
            "#loc26 = loc(\"run_cell\"(#loc8))\n",
            "#loc27 = loc(\"run_cell\"(#loc9))\n",
            "#loc30 = loc(\"__call__\"(#loc13))\n",
            "#loc32 = loc(\"__call__\"(#loc15))\n",
            "#loc37 = loc(callsite(#loc26 at #loc27))\n",
            "#loc39 = loc(callsite(#loc25 at #loc37))\n",
            "#loc41 = loc(callsite(#loc24 at #loc39))\n",
            "#loc43 = loc(callsite(#loc23 at #loc41))\n",
            "#loc45 = loc(callsite(#loc22 at #loc43))\n",
            "#loc47 = loc(callsite(#loc21 at #loc45))\n",
            "#loc49 = loc(callsite(#loc20 at #loc47))\n",
            "#loc51 = loc(callsite(#loc19 at #loc49))\n",
            "#loc54 = loc(callsite(#loc30 at #loc51))\n",
            "#loc56 = loc(callsite(#loc32 at #loc51))\n",
            "#loc65 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))\n",
            "#loc68 = loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc56))\n",
            "module @jit__unnamed_wrapped_function_ attributes {jax.uses_shape_polymorphism = true, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<?x3x224x224xf32> loc(unknown)) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc)\n",
            "    %0 = stablehlo.get_dimension_size %arg0, dim = 0 : (tensor<?x3x224x224xf32>) -> tensor<i32> loc(#loc60)\n",
            "    %1 = stablehlo.compare  GE, %0, %c,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc61)\n",
            "    stablehlo.custom_call @shape_assertion(%1, %0) {api_version = 2 : i32, error_message = \"Input shapes do not match the polymorphic shapes specification. Expected value >= 1 for dimension variable 'a'. Using the following polymorphic shapes specifications: args[0].shape = (a, 3, 224, 224). Obtained dimension variables: 'a' = {0} from specification 'a' for dimension args[0].shape[0] (= {0}), . Please see https://jax.readthedocs.io/en/latest/export/shape_poly.html#shape-assertion-errors for more details.\", has_side_effect = true} : (tensor<i1>, tensor<i32>) -> () loc(#loc62)\n",
            "    %2 = call @_wrapped_jax_export_main(%0, %arg0) : (tensor<i32>, tensor<?x3x224x224xf32>) -> tensor<?x10xf32> loc(#loc)\n",
            "    return %2 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @_wrapped_jax_export_main(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(unknown), %arg1: tensor<?x3x224x224xf32> loc(\"x\")) -> (tensor<?x10xf32> {jax.result_info = \"\"}) {\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %c = stablehlo.constant dense<10> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<1> : tensor<1xi32> loc(#loc)\n",
            "    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)\n",
            "    %cst_2 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc)\n",
            "    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<32x3x3x3xf32> loc(#loc63)\n",
            "    %1 = stablehlo.convolution(%arg1, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x3x224x224xf32>, tensor<32x3x3x3xf32>) -> tensor<?x32x224x224xf32> loc(#loc64)\n",
            "    %2 = call @relu(%arg0, %1) : (tensor<i32>, tensor<?x32x224x224xf32>) -> tensor<?x32x224x224xf32> loc(#loc65)\n",
            "    %3 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<64x32x3x3xf32> loc(#loc66)\n",
            "    %4 = stablehlo.convolution(%2, %3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x32x224x224xf32>, tensor<64x32x3x3xf32>) -> tensor<?x64x224x224xf32> loc(#loc67)\n",
            "    %5 = call @relu_0(%arg0, %4) : (tensor<i32>, tensor<?x64x224x224xf32>) -> tensor<?x64x224x224xf32> loc(#loc68)\n",
            "    %6 = stablehlo.reduce(%5 init: %cst_1) applies stablehlo.maximum across dimensions = [1, 2] : (tensor<?x64x224x224xf32>, tensor<f32>) -> tensor<?x224xf32> loc(#loc69)\n",
            "    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<224x10xf32> loc(#loc70)\n",
            "    %8 = stablehlo.dot_general %6, %7, contracting_dims = [1] x [0] : (tensor<?x224xf32>, tensor<224x10xf32>) -> tensor<?x10xf32> loc(#loc71)\n",
            "    %9 = stablehlo.reduce(%8 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc72)\n",
            "    %10 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc73)\n",
            "    %11 = stablehlo.dynamic_broadcast_in_dim %cst_1, %10, dims = [] : (tensor<f32>, tensor<1xi32>) -> tensor<?xf32> loc(#loc73)\n",
            "    %12 = stablehlo.maximum %11, %9 : tensor<?xf32> loc(#loc73)\n",
            "    %13 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc74)\n",
            "    %14 = stablehlo.concatenate %13, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc74)\n",
            "    %15 = stablehlo.dynamic_broadcast_in_dim %12, %14, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc74)\n",
            "    %16 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc75)\n",
            "    %17 = stablehlo.concatenate %16, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc75)\n",
            "    %18 = stablehlo.dynamic_broadcast_in_dim %15, %17, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc75)\n",
            "    %19 = stablehlo.subtract %8, %18 : tensor<?x10xf32> loc(#loc75)\n",
            "    %20 = stablehlo.exponential %19 : tensor<?x10xf32> loc(#loc76)\n",
            "    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?xf32> loc(#loc77)\n",
            "    %22 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc74)\n",
            "    %23 = stablehlo.concatenate %22, %c_0, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc74)\n",
            "    %24 = stablehlo.dynamic_broadcast_in_dim %21, %23, dims = [0] : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x1xf32> loc(#loc74)\n",
            "    %25 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc78)\n",
            "    %26 = stablehlo.concatenate %25, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32> loc(#loc78)\n",
            "    %27 = stablehlo.dynamic_broadcast_in_dim %24, %26, dims = [0, 1] : (tensor<?x1xf32>, tensor<2xi32>) -> tensor<?x10xf32> loc(#loc78)\n",
            "    %28 = stablehlo.divide %20, %27 : tensor<?x10xf32> loc(#loc78)\n",
            "    return %28 : tensor<?x10xf32> loc(#loc)\n",
            "  } loc(#loc)\n",
            "  func.func private @relu(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54)), %arg1: tensor<?x32x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc54))) -> tensor<?x32x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<32> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc65)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc79)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc79)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x32x224x224xf32> loc(#loc79)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x32x224x224xf32> loc(#loc79)\n",
            "    return %3 : tensor<?x32x224x224xf32> loc(#loc65)\n",
            "  } loc(#loc65)\n",
            "  func.func private @relu_0(%arg0: tensor<i32> {jax.global_constant = \"a\"} loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc56)), %arg1: tensor<?x64x224x224xf32> loc(\"jit(<unnamed wrapped function>)/jit(main)/pjit\"(#loc56))) -> tensor<?x64x224x224xf32> {\n",
            "    %c = stablehlo.constant dense<224> : tensor<1xi32> loc(#loc)\n",
            "    %c_0 = stablehlo.constant dense<64> : tensor<1xi32> loc(#loc)\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc68)\n",
            "    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32> loc(#loc80)\n",
            "    %1 = stablehlo.concatenate %0, %c_0, %c, %c, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32> loc(#loc80)\n",
            "    %2 = stablehlo.dynamic_broadcast_in_dim %cst, %1, dims = [] : (tensor<f32>, tensor<4xi32>) -> tensor<?x64x224x224xf32> loc(#loc80)\n",
            "    %3 = stablehlo.maximum %arg1, %2 : tensor<?x64x224x224xf32> loc(#loc80)\n",
            "    return %3 : tensor<?x64x224x224xf32> loc(#loc68)\n",
            "  } loc(#loc68)\n",
            "} loc(#loc)\n",
            "#loc10 = loc(\"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\":302:0)\n",
            "#loc12 = loc(\"<ipython-input-2-2e537abe9551>\":82:0)\n",
            "#loc14 = loc(\"<ipython-input-2-2e537abe9551>\":84:0)\n",
            "#loc16 = loc(\"<ipython-input-2-2e537abe9551>\":86:0)\n",
            "#loc17 = loc(\"<ipython-input-2-2e537abe9551>\":88:0)\n",
            "#loc18 = loc(\"<ipython-input-2-2e537abe9551>\":89:0)\n",
            "#loc28 = loc(\"do_execute\"(#loc10))\n",
            "#loc29 = loc(\"__call__\"(#loc12))\n",
            "#loc31 = loc(\"__call__\"(#loc14))\n",
            "#loc33 = loc(\"__call__\"(#loc16))\n",
            "#loc34 = loc(\"__call__\"(#loc17))\n",
            "#loc35 = loc(\"__call__\"(#loc18))\n",
            "#loc36 = loc(callsite(#loc27 at #loc28))\n",
            "#loc38 = loc(callsite(#loc26 at #loc36))\n",
            "#loc40 = loc(callsite(#loc25 at #loc38))\n",
            "#loc42 = loc(callsite(#loc24 at #loc40))\n",
            "#loc44 = loc(callsite(#loc23 at #loc42))\n",
            "#loc46 = loc(callsite(#loc22 at #loc44))\n",
            "#loc48 = loc(callsite(#loc21 at #loc46))\n",
            "#loc50 = loc(callsite(#loc20 at #loc48))\n",
            "#loc52 = loc(callsite(#loc19 at #loc50))\n",
            "#loc53 = loc(callsite(#loc29 at #loc51))\n",
            "#loc55 = loc(callsite(#loc31 at #loc51))\n",
            "#loc57 = loc(callsite(#loc33 at #loc51))\n",
            "#loc58 = loc(callsite(#loc34 at #loc51))\n",
            "#loc59 = loc(callsite(#loc35 at #loc51))\n",
            "#loc60 = loc(\"/dimension_size\"(#loc52))\n",
            "#loc61 = loc(\"/ge\"(#loc52))\n",
            "#loc62 = loc(\"/shape_assertion\"(#loc52))\n",
            "#loc63 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc53))\n",
            "#loc64 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc53))\n",
            "#loc66 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc55))\n",
            "#loc67 = loc(\"jit(<unnamed wrapped function>)/jit(main)/conv_general_dilated\"(#loc55))\n",
            "#loc69 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc57))\n",
            "#loc70 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc58))\n",
            "#loc71 = loc(\"jit(<unnamed wrapped function>)/jit(main)/dot_general\"(#loc58))\n",
            "#loc72 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_max\"(#loc59))\n",
            "#loc73 = loc(\"jit(<unnamed wrapped function>)/jit(main)/max\"(#loc59))\n",
            "#loc74 = loc(\"jit(<unnamed wrapped function>)/jit(main)/broadcast_in_dim\"(#loc59))\n",
            "#loc75 = loc(\"jit(<unnamed wrapped function>)/jit(main)/sub\"(#loc59))\n",
            "#loc76 = loc(\"jit(<unnamed wrapped function>)/jit(main)/exp\"(#loc59))\n",
            "#loc77 = loc(\"jit(<unnamed wrapped function>)/jit(main)/reduce_sum\"(#loc59))\n",
            "#loc78 = loc(\"jit(<unnamed wrapped function>)/jit(main)/div\"(#loc59))\n",
            "#loc79 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc54))\n",
            "#loc80 = loc(\"jit(<unnamed wrapped function>)/jit(main)/jit(relu)/max\"(#loc56))\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRIu7xlSoDUK"
      },
      "source": [
        "A few things to note in the exported StableHLO:\n",
        "\n",
        "1. The exported program now has `tensor<?x3x224x224xf32>`. These input types can be refined in many ways: StableHLO has APIs to [refine shapes](https://github.com/openxla/stablehlo/blob/541db997e449dcfee8536043dfdd49bb13f9ed1a/stablehlo/transforms/Passes.td#L69-L99) and [canonicalize dynamic programs](https://github.com/openxla/stablehlo/blob/541db997e449dcfee8536043dfdd49bb13f9ed1a/stablehlo/transforms/Passes.td#L18-L28) to static programs. TensorFlow SavedModel execution also takes care of refinement which we'll see in the next example.\n",
        "2. JAX will generate guards to ensure the values of `a` are valid, in this case `a > 1` is checked. These can be washed away at compile time once refined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFU5M6Xm1U8_"
      },
      "source": [
        "## Export to TensorFlow SavedModel\n",
        "\n",
        "It is common to export a StableHLO model to SavedModel for interoperability with existing compilation pipelines, existing TensorFlow tooling, or serving via [TensorFlow Serving](https://github.com/tensorflow/serving).\n",
        "\n",
        "JAX makes it easy to pack StableHLO into a SavedModel, and load that SavedModel in the future. For this section, we'll be using our dynamic model from the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf7Fnsrop7BD"
      },
      "source": [
        "### Export to SavedModel using `jax2tf`\n",
        "\n",
        "JAX provides a simple API for exporting StableHLO into a format that can be packaged in SavedModel in `jax.experimental.jax2tf`. This uses the `export` function under the hood, so the same `jit` requirements apply.\n",
        "\n",
        "Full details on `jax2tf` can be found in the [README](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#jax-and-tensorflow-interoperation-jax2tfcall_tf). For this example, we'll only need to know the `polymorphic_shapes` option to specify our dynamic batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXkgtX7QEiWa",
        "outputId": "9034836e-4ba1-4210-c2c1-316777d4ad89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34massets\u001b[m\u001b[m         fingerprint.pb saved_model.pb \u001b[34mvariables\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "from jax.experimental import jax2tf\n",
        "import tensorflow as tf\n",
        "\n",
        "exported_f = jax2tf.convert(resnet18, polymorphic_shapes=[\"(a,3,224,224)\"])\n",
        "\n",
        "# Copied from the jax2tf README.md > Usage: saved model\n",
        "my_model = tf.Module()\n",
        "my_model.f = tf.function(exported_f, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 224, 224], tf.float32))\n",
        "tf.saved_model.save(my_model, '/tmp/resnet18_tf', options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n",
        "\n",
        "!ls /tmp/resnet18_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmKABmLdrS3C"
      },
      "source": [
        "### Reload and call the SavedModel\n",
        "\n",
        "Now we can load that SavedModel and compile using our `sample_input` from a previous example.\n",
        "\n",
        "_Note: The restored model does *not* require JAX to run, just XLA._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Az3dXXWrVDM",
        "outputId": "cac6d3b1-126e-4e66-dc4d-f2a798ede463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result shape: (1, 512, 7, 7)\n"
          ]
        }
      ],
      "source": [
        "restored_model = tf.saved_model.load('/tmp/resnet18_tf')\n",
        "restored_result = restored_model.f(tf.constant(sample_input, tf.float32))\n",
        "print(\"Result shape:\", restored_result[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Dsm2oF5jn4"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### `jax.jit` issues\n",
        "\n",
        "If the function can be JIT'ed, then it can be exported. Ensure `jax.jit` works first, or look in desired project for uses of JIT already (for example, [AlphaFold's `apply`](https://github.com/google-deepmind/alphafold/blob/dbe2a438ebfc6289f960292f15dbf421a05e563d/alphafold/model/model.py#L89) can be exported easily).\n",
        "\n",
        "See [JAX's JIT compilation documentation](https://jax.readthedocs.io/en/latest/jit-compilation.html) and [`jax.jit` API reference and examples](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) for troubleshooting JIT transformations. The most common issue is control flow, which can often be resolved with `static_argnums` / `static_argnames` as in the linked example.\n",
        "\n",
        "### Support tickets\n",
        "\n",
        "You can open an issue on GitHub for further help. Include a reproducible example using one of the above APIs in your issue report, this will help get the issue resolved much quicker!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "380c81f382154e6ba3efe095c67b144a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b1f6ab4339e47bc9e38fb5761f18979",
              "IPY_MODEL_050c8a5cde1242caabb38df30b17c1c1",
              "IPY_MODEL_d5e540c6357f48a7ab3b2b94d83cd134"
            ],
            "layout": "IPY_MODEL_cf356b245daf4e76ad1ce004a6484dd5"
          }
        },
        "2b1f6ab4339e47bc9e38fb5761f18979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87df9af2b5444bdcab73d33264963cec",
            "placeholder": "​",
            "style": "IPY_MODEL_7b674244d0a74b9b9e6816e501b16c3e",
            "value": "config.json: 100%"
          }
        },
        "050c8a5cde1242caabb38df30b17c1c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba2bbca105b84e5fb49a47aa26cf1d48",
            "max": 69548,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e6483653aa74c0bb40f2fbafa1c9f11",
            "value": 69548
          }
        },
        "d5e540c6357f48a7ab3b2b94d83cd134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c2352d91e674be897d4a43624ba7d63",
            "placeholder": "​",
            "style": "IPY_MODEL_50da9b3e450d435a9df7a79dd110c2de",
            "value": " 69.5k/69.5k [00:00&lt;00:00, 3.81MB/s]"
          }
        },
        "cf356b245daf4e76ad1ce004a6484dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87df9af2b5444bdcab73d33264963cec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b674244d0a74b9b9e6816e501b16c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba2bbca105b84e5fb49a47aa26cf1d48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e6483653aa74c0bb40f2fbafa1c9f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c2352d91e674be897d4a43624ba7d63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50da9b3e450d435a9df7a79dd110c2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "013de714e91b4aba9ac0a16d83758ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3921d6b491154a52aa7aaef66b4bfc7e",
              "IPY_MODEL_8446d52500da43a7986ab2b7f8d03020",
              "IPY_MODEL_7a06f00059f943598b78808a61892d04"
            ],
            "layout": "IPY_MODEL_ea38f6b6aca94f298899c2c27d675a02"
          }
        },
        "3921d6b491154a52aa7aaef66b4bfc7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a615e90cf5d434882a55731eb7ff029",
            "placeholder": "​",
            "style": "IPY_MODEL_d1e21d9b12d145d5ad87319a1691e1d6",
            "value": "model.safetensors: 100%"
          }
        },
        "8446d52500da43a7986ab2b7f8d03020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098f52f4561b46a89cd0d48bb9889379",
            "max": 46812324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff0d371c27334310b0dcf1d6496c24a3",
            "value": 46812324
          }
        },
        "7a06f00059f943598b78808a61892d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_595dd5d2b8774798b4b96fcf64ae8478",
            "placeholder": "​",
            "style": "IPY_MODEL_c9883e3cceab4370a88a1989cc17c8a2",
            "value": " 46.8M/46.8M [00:00&lt;00:00, 141MB/s]"
          }
        },
        "ea38f6b6aca94f298899c2c27d675a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a615e90cf5d434882a55731eb7ff029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e21d9b12d145d5ad87319a1691e1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "098f52f4561b46a89cd0d48bb9889379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0d371c27334310b0dcf1d6496c24a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "595dd5d2b8774798b4b96fcf64ae8478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9883e3cceab4370a88a1989cc17c8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}